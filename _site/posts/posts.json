[
  {
    "path": "posts/deutschland-tour-2024/",
    "title": "Deutschland Tour 2024",
    "description": "Exploring the route of this years 'Deutschland Tour' with R",
    "author": [
      {
        "name": "Julian During",
        "url": {}
      }
    ],
    "date": "2024-08-07",
    "categories": [],
    "contents": "\r\nIdea\r\nThe “Deutschland Tour” is a big (road) bike race in Germany.\r\nBy far not as big as the “Tour de France”, but maybe it will get there :).\r\nThis years “Deutschland Tour” is a special one. It passes near my hometown and\r\ntherefore I am wondering where there is a good spot for spectators.\r\nFor me these key points\r\nare important:\r\nNot too far away\r\nNice mountain climb (otherwise the riders pass very quickly)\r\nNearby drinks are welcome :)\r\nTo answer this question, I will use my R skills to import and visualise the\r\ndata.\r\nReproducibility\r\nIn this analysis the following libraries are used:\r\n\r\n\r\nlibrary(tarchetypes)\r\nlibrary(conflicted)\r\nlibrary(prettymapr)\r\nlibrary(tidyverse)\r\nlibrary(ggspatial)\r\nlibrary(leaflet)\r\nlibrary(targets)\r\nlibrary(httr2)\r\nlibrary(xml2)\r\nlibrary(fs)\r\nlibrary(sf)\r\n\r\nconflicts_prefer(dplyr::filter)\r\n\r\n\r\nIf you want to reproduce this analysis, you have to perform the following steps:\r\nClone the repository (see footnote)\r\nRun renv::restore() (Ushey and Wickham 2024)\r\nRun targets::tar_make() (Landau 2021)\r\nAlternatively, you could run this analysis by copying and executing chunk by\r\nchunk in your R session (installing the above mentioned packages manually).\r\nData\r\nAt first define where gpx files can be read from:\r\n\r\n\r\ngpx_url <- c(\"https://www.deutschland-tour.com/fileadmin/content/2_Deutschland_Tour/DT_24/Elite/DT24_E1_SW-HN_177km_inklneutral.gpx\", \r\n     \"https://www.deutschland-tour.com/fileadmin/content/2_Deutschland_Tour/DT_24/Elite/DT24_E2_HN-GD_173km_inklneutral.gpx\", \r\n     \"https://www.deutschland-tour.com/fileadmin/content/2_Deutschland_Tour/DT_24/Elite/DT24_E3_GD-VS_211km_inklneutral.gpx\", \r\n     \"https://www.deutschland-tour.com/fileadmin/content/2_Deutschland_Tour/DT_24/Elite/DT24_E4_Annw-SB_182km_inklneutral.gpx\")\r\n\r\n\r\nDefine helper function that reads in stage data. To do this first download\r\nthe gpx file (Wickham 2024) and save it to an temporary file (Hester, Wickham, and Csárdi 2024).\r\nThe resulting file is a xml file. Read it in (Wickham, Hester, and Ooms 2023)\r\nand get all the elements that represent “trackpoints”.\r\nExtract spatial information from this “trackpoints” using various\r\nhelper functions (see documentation links):\r\n\r\n\r\nstage <- function(gpx_url) {\r\n  res_path <- file_temp()\r\n  \r\n  req_perform(request(gpx_url), path = res_path)\r\n  \r\n  gpx_xml <- read_xml(res_path)\r\n  \r\n  gpx_trackpoints <- gpx_xml |>\r\n    xml_child(1) |>\r\n    xml_child(2) |>\r\n    xml_children()\r\n  \r\n  tibble(\r\n    lat = xml_attr(gpx_trackpoints, \"lat\"),\r\n    lon = xml_attr(gpx_trackpoints, \"lon\"),\r\n    elevation = xml_text(xml_children(gpx_trackpoints))) |>\r\n    mutate(point_nr = row_number())\r\n}\r\n\r\n\r\nApply the above mentioned function to all urls resulting in one final data\r\nframe:\r\n\r\n\r\ndf_stages <- map_df(gpx_url, function(x) stage(x), .id = \"stage_id\")\r\n\r\n\r\nAnalysis\r\nTurn data frame into a sf (Pebesma 2018) object:\r\n\r\n\r\nsf_stages <- st_as_sf(df_stages, coords = c(\"lon\", \"lat\"), crs = st_crs(4326))\r\n\r\n\r\n\r\nSimple feature collection with 40834 features and 3 fields\r\nGeometry type: POINT\r\nDimension:     XY\r\nBounding box:  xmin: 6.97561 ymin: 48.05465 xmax: 10.25127 ymax: 50.04494\r\nGeodetic CRS:  WGS 84\r\n# A tibble: 40,834 × 4\r\n   stage_id elevation point_nr            geometry\r\n * <chr>    <chr>        <int>         <POINT [°]>\r\n 1 1        238.2            1 (10.23479 50.04494)\r\n 2 1        236.1            2 (10.23523 50.04476)\r\n 3 1        236.1            3 (10.23523 50.04476)\r\n 4 1        233.0            4 (10.23582 50.04443)\r\n 5 1        233.0            5 (10.23582 50.04443)\r\n 6 1        232.9            6 (10.23637 50.04409)\r\n 7 1        232.9            7 (10.23637 50.04409)\r\n 8 1        226.9            8 (10.23682 50.04377)\r\n 9 1        226.9            9 (10.23682 50.04377)\r\n10 1        227.6           10 (10.23678 50.04381)\r\n# ℹ 40,824 more rows\r\n\r\nThe spatial data is represented as points at the moment.\r\nSummarise points per stage, combining them into ‘multipoints’ and one row\r\nper stage:\r\n\r\n\r\nsf_stages_multipoint <- summarise(sf_stages, geometry = st_combine(geometry), \r\n     .by = stage_id)\r\n\r\n\r\n\r\nSimple feature collection with 4 features and 1 field\r\nGeometry type: MULTIPOINT\r\nDimension:     XY\r\nBounding box:  xmin: 6.97561 ymin: 48.05465 xmax: 10.25127 ymax: 50.04494\r\nGeodetic CRS:  WGS 84\r\n# A tibble: 4 × 2\r\n  stage_id                                                    geometry\r\n  <chr>                                               <MULTIPOINT [°]>\r\n1 1        ((10.23479 50.04494), (10.23523 50.04476), (10.23523 50.04…\r\n2 2        ((9.21813 49.1415), (9.21785 49.14149), (9.21706 49.14149)…\r\n3 3        ((9.79703 48.80134), (9.79703 48.80133), (9.7971 48.80137)…\r\n4 4        ((7.96211 49.20398), (7.96207 49.2041), (7.96204 49.20418)…\r\n\r\nCast into lines with this operation:\r\n\r\n\r\nsf_stages_line <- st_cast(sf_stages_multipoint, \"LINESTRING\")\r\n\r\n\r\n\r\nSimple feature collection with 4 features and 1 field\r\nGeometry type: LINESTRING\r\nDimension:     XY\r\nBounding box:  xmin: 6.97561 ymin: 48.05465 xmax: 10.25127 ymax: 50.04494\r\nGeodetic CRS:  WGS 84\r\n# A tibble: 4 × 2\r\n  stage_id                                                    geometry\r\n  <chr>                                               <LINESTRING [°]>\r\n1 1        (10.23479 50.04494, 10.23523 50.04476, 10.23523 50.04476, …\r\n2 2        (9.21813 49.1415, 9.21785 49.14149, 9.21706 49.14149, 9.21…\r\n3 3        (9.79703 48.80134, 9.79703 48.80133, 9.7971 48.80137, 9.79…\r\n4 4        (7.96211 49.20398, 7.96207 49.2041, 7.96204 49.20418, 7.96…\r\n\r\nWe can now plot the data using known ‘tidyverse’ (Wickham et al. 2019) techniques.\r\nTo Include an underlying map, ‘ggspatial’ (Dunnington 2023) is used.\r\n\r\n\r\nvis_stages_line <- function(sf_stages_line) {\r\n  ggplot() +\r\n    annotation_map_tile(zoom = 8, type = \"cartolight\") +\r\n    layer_spatial(sf_stages_line, aes(color = stage_id)) +\r\n    theme(legend.position = \"bottom\") +\r\n    labs(\r\n      color = \"Stage Number\",\r\n      title = \"Deutschland Tour 2024\",\r\n      subtitle = \"Color indicates Stage Number\")\r\n}\r\n\r\n\r\n\r\n\r\ngg_stages_line <- vis_stages_line(sf_stages_line)\r\n\r\n\r\n\r\n\r\n\r\nWith ‘leaflet’ (Cheng et al. 2024) we can also have an interactive look:\r\n\r\n\r\nvis_stages_line_interactive <- function(sf_stages_line) {\r\n  sf_vis <- sf_stages_line |>\r\n    mutate(stage_id = as_factor(stage_id))\r\n  \r\n  factpal <- colorFactor(topo.colors(nrow(sf_vis)), sf_vis$stage_id)\r\n  \r\n  leaflet(sf_vis) |>\r\n    addPolylines(color = ~factpal(stage_id)) |>\r\n    addTiles()\r\n}\r\n\r\n\r\n\r\n\r\ngg_stages_line_interactive <- vis_stages_line_interactive(sf_stages_line)\r\n\r\n\r\n\r\n\r\n\r\nConclusion\r\nI think I found my perfect spot. By downloading the data, turning it into\r\na spatial format and plotting it interactively, I could easily explore the\r\nroute. I hope this helps you to find your spot as well! Hope to see you there :)\r\n\r\n\r\n\r\nCheng, Joe, Barret Schloerke, Bhaskar Karambelkar, and Yihui Xie. 2024. Leaflet: Create Interactive Web Maps with the JavaScript ’Leaflet’ Library. https://CRAN.R-project.org/package=leaflet.\r\n\r\n\r\nDunnington, Dewey. 2023. Ggspatial: Spatial Data Framework for Ggplot2. https://CRAN.R-project.org/package=ggspatial.\r\n\r\n\r\nHester, Jim, Hadley Wickham, and Gábor Csárdi. 2024. Fs: Cross-Platform File System Operations Based on ’Libuv’. https://CRAN.R-project.org/package=fs.\r\n\r\n\r\nLandau, William Michael. 2021. “The Targets r Package: A Dynamic Make-Like Function-Oriented Pipeline Toolkit for Reproducibility and High-Performance Computing.” Journal of Open Source Software 6 (57): 2959. https://doi.org/10.21105/joss.02959.\r\n\r\n\r\nPebesma, Edzer. 2018. “Simple Features for R: Standardized Support for Spatial Vector Data.” The R Journal 10 (1): 439–46. https://doi.org/10.32614/RJ-2018-009.\r\n\r\n\r\nUshey, Kevin, and Hadley Wickham. 2024. Renv: Project Environments. https://CRAN.R-project.org/package=renv.\r\n\r\n\r\nWickham, Hadley. 2024. Httr2: Perform HTTP Requests and Process the Responses. https://CRAN.R-project.org/package=httr2.\r\n\r\n\r\nWickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy D’Agostino McGowan, Romain François, Garrett Grolemund, et al. 2019. “Welcome to the tidyverse.” Journal of Open Source Software 4 (43): 1686. https://doi.org/10.21105/joss.01686.\r\n\r\n\r\nWickham, Hadley, Jim Hester, and Jeroen Ooms. 2023. Xml2: Parse XML. https://CRAN.R-project.org/package=xml2.\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/deutschland-tour-2024/distill-preview.png",
    "last_modified": "2024-08-07T21:46:45+02:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/getting-over-it/",
    "title": "Getting Over It",
    "description": "Visualising my Transalp bike ride",
    "author": [
      {
        "name": "Julian During",
        "url": {}
      }
    ],
    "date": "2024-04-05",
    "categories": [],
    "contents": "\r\nIn summer 2020, I crossed the alps with my road bike.\r\nI’ve recorded the whole ride\r\nand as a nice memory, I would like to visualise this ride.\r\nData\r\nLoad the other necessary libraries:\r\n\r\n\r\nlibrary(tarchetypes)\r\nlibrary(conflicted)\r\nlibrary(patchwork)\r\nlibrary(tidyterra)\r\nlibrary(tidyverse)\r\nlibrary(elevatr)\r\nlibrary(ggrepel)\r\nlibrary(targets)\r\nlibrary(raster)\r\nlibrary(scales)\r\nlibrary(terra)\r\nlibrary(pins)\r\nlibrary(fs)\r\nlibrary(sf)\r\n\r\nconflicts_prefer(dplyr::filter)\r\nconflicts_prefer(dplyr::lag)\r\nconflicts_prefer(dplyr::select)\r\n\r\n\r\nDefine where activity data is read from:\r\n\r\n\r\nact_file <- \"data/act.rds\"\r\n\r\n\r\n\r\n\r\nmeas_file <- \"data/meas.rds\"\r\n\r\n\r\nRead activity data from rds file. Define point of interests names and\r\ndefine if it should be mapped to the start or the end of the activity:\r\n\r\n\r\nact <- function(act_file) {\r\n  read_rds(act_file) |>\r\n    mutate(\r\n      start_name = case_when(\r\n        id == \"3650448726\" ~ \"Albstadt\",\r\n        id == \"3654245140\" ~ \"Winterthur\",\r\n        id == \"3659045033\" ~ \"Fluelen\",\r\n        id == \"3664650034\" ~ \"Andermatt\",\r\n        id == \"3669729902\" ~ \"Andermatt\",\r\n        TRUE ~ NA_character_),\r\n      end_name = case_when(\r\n        id == \"3650448726\" ~ \"Winterthur\",\r\n        id == \"3654245140\" ~ \"Fluelen\",\r\n        id == \"3659045033\" ~ \"Andermatt\",\r\n        id == \"3664650034\" ~ \"Andermatt\",\r\n        id == \"3669729902\" ~ \"Lugano\",\r\n        TRUE ~ NA_character_),\r\n      poi_name = if_else(end_name == \"Lugano\", \"Lugano\", start_name),\r\n      name = str_glue(\"{start_name} - {end_name}\"),\r\n      lat_poi = if_else(\r\n        poi_name == \"Lugano\",\r\n        map_dbl(end_latlng, 1),\r\n        map_dbl(start_latlng, 1)),\r\n      lng_poi = if_else(\r\n        poi_name == \"Lugano\",\r\n        map_dbl(end_latlng, 2),\r\n        map_dbl(start_latlng, 2))) |>\r\n    select(-distance)\r\n}\r\n\r\n\r\n\r\n\r\ndf_act <- act(act_file)\r\n\r\n\r\nRead meas data:\r\n\r\n\r\nmeas <- function(meas_file) {\r\n  df_meas_raw <- read_rds(meas_file)\r\n\r\n  df_distance_max <- df_meas_raw |>\r\n    group_by(id) |>\r\n    summarise(distance_max = max(distance)) |>\r\n    mutate(\r\n      distance_max = lag(distance_max, default = 0),\r\n      distance_max = cumsum(distance_max))\r\n\r\n  df_meas_raw |>\r\n    left_join(df_distance_max, by = join_by(id)) |>\r\n    group_by(id) |>\r\n    mutate(distance_cum = distance + distance_max) |>\r\n    ungroup()\r\n}\r\n\r\n\r\n\r\n\r\ndf_meas <- meas(meas_file)\r\n\r\n\r\nTurn measurements into sf object:\r\n\r\n\r\nact_meas_points <- function(df_meas, df_act) {\r\n  df_meas |>\r\n    st_as_sf(coords = c(\"lng\", \"lat\"), crs = 4326) |>\r\n    left_join(df_act, by = join_by(id))\r\n}\r\n\r\n\r\n\r\n\r\nsf_act_meas_points <- act_meas_points(df_meas, df_act)\r\n\r\n\r\nCombine into one LINESTRING:\r\n\r\n\r\nact_meas_lines <- function(sf_act_meas_points) {\r\n  sf_act_meas_points |>\r\n    group_by(id, name, lng_poi, lat_poi, poi_name) |>\r\n    summarise(\r\n      mean_speed = mean(velocity_smooth),\r\n      mean_altitude = mean(altitude),\r\n      mean_distance = mean(distance_cum),\r\n      do_union = FALSE, .groups = \"drop\") |>\r\n    st_cast(\"LINESTRING\")\r\n}\r\n\r\n\r\n\r\n\r\nsf_act_meas_lines <- act_meas_lines(sf_act_meas_points)\r\n\r\n\r\nRead raster data:\r\n\r\n\r\nalpen_raster <- function(sf_lines) {\r\n  get_elev_raster(sf_lines, z = 6, clip = \"bbox\", expand = 0.1) |>\r\n    rast() |>\r\n    wrap()\r\n}\r\n\r\n\r\n\r\n\r\nraster_alpen <- alpen_raster(sf_act_meas_lines)\r\n\r\n\r\nVisualisation\r\n\r\n\r\nvis_ride <- function(sf_act_meas_lines, sf_act_meas_points, raster_alpen) {\r\n  theme_set(theme_light(base_size = 12))\r\n\r\n  raster_alpen_unpacked <- raster_alpen |>\r\n    unwrap()\r\n\r\n  gg_ride <- ggplot(sf_act_meas_lines) +\r\n    geom_spatraster_contour(data = raster_alpen_unpacked, binwidth = 200) +\r\n    geom_sf(aes(color = id)) +\r\n    geom_label_repel(\r\n      aes(x = lng_poi, y = lat_poi, label = poi_name),\r\n      size = 2.5, fill = alpha(c(\"white\"), 0.5)) +\r\n    labs(\r\n      x = \"Longitude\", y = \"Latitude\") +\r\n    theme(\r\n      legend.position = \"none\", panel.grid.major = element_blank(),\r\n      axis.text.x = element_text(angle = 90))\r\n\r\n  gg_altitude <- sf_act_meas_points |>\r\n    filter(moving) |>\r\n    ggplot(aes(x = distance_cum, y = altitude, color = id, group = id)) +\r\n    geom_line() +\r\n    geom_label(\r\n      data = sf_act_meas_lines,\r\n      mapping = aes(x = mean_distance, y = mean_altitude, label = name),\r\n      fill = alpha(c(\"white\"), 0.5)) +\r\n    labs(x = \"Distance [km]\", y = \"Height [m]\") +\r\n    theme(legend.position = \"none\") +\r\n    expand_limits(y = 0) +\r\n    scale_x_continuous(\r\n      labels = label_number(scale = 0.001),\r\n      breaks = breaks_width(50000)) +\r\n    scale_y_continuous(position = \"right\")\r\n\r\n  gg_final <- gg_ride + gg_altitude +\r\n    plot_layout(widths = c(1, 3)) +\r\n    plot_annotation(\r\n      title = \"Transalp 2020\",\r\n      subtitle = \"Albstadt - Lugano\") &\r\n    theme(text = element_text(family = \"Fira Code\", size = 12))\r\n\r\n  ggsave(\r\n    \"transalp.png\", gg_final,\r\n    width = 40, height = 25, units = \"cm\", scale = 0.8)\r\n}\r\n\r\n\r\n\r\n\r\ngg_ride <- vis_ride(sf_act_meas_lines, sf_act_meas_points, raster_alpen)\r\n\r\n\r\n\r\n\r\n\r\nWhat a ride it has been! I’m always thinking back to this experience.\r\nIt’s time to recreate such a ride in the near future.\r\n\r\n\r\n\r\n",
    "preview": "posts/getting-over-it/distill-preview.png",
    "last_modified": "2024-04-05T23:01:05+02:00",
    "input_file": {},
    "preview_width": 3779,
    "preview_height": 2362
  },
  {
    "path": "posts/friendly-webscraping/",
    "title": "Friendly Webscraping",
    "description": "Scraping my local Football Club's News Data",
    "author": [
      {
        "name": "Julian During",
        "url": {}
      }
    ],
    "date": "2024-03-28",
    "categories": [],
    "contents": "\r\nIdea\r\nScrape the website of my local football club to get an overview\r\nof the content there.\r\nThe CSS selectors were extracted using techniques described in this\r\nwonderful tutorial.\r\nMainly relying on the developer features of your web browser.\r\nIf you want to reproduce this analysis, you have to perform the following steps:\r\nClone the repository\r\nRun renv::restore()\r\nRun targets::tar_make()\r\nData\r\nThe following libraries are used in this analysis:\r\n\r\n\r\nlibrary(ggwordcloud)\r\nlibrary(tarchetypes)\r\nlibrary(conflicted)\r\nlibrary(wordcloud)\r\nlibrary(tidyverse)\r\nlibrary(tidytext)\r\nlibrary(distill)\r\nlibrary(targets)\r\nlibrary(assertr)\r\nlibrary(polite)\r\nlibrary(httr2)\r\nlibrary(rvest)\r\nlibrary(fs)\r\n\r\nconflicts_prefer(dplyr::filter)\r\n\r\n\r\nDefine where to look for the data:\r\n\r\n\r\ntsg_url <- \"https://www.tsg-fussball.de/\"\r\n\r\n\r\nWe want to obey the scraping restrictions defined by the host.\r\nTherefore, we introduce ourselves to the host and follow the restrictions\r\ndefined in ‘robots.txt’. This can be done using the bow function\r\nfrom the polite package:\r\n\r\n\r\ntsg_host <- bow(tsg_url)\r\n\r\n\r\nThese are the following for this example:\r\n\r\n<polite session> https://www.tsg-fussball.de/\r\n    User-agent: polite R package\r\n    robots.txt: 1 rules are defined for 1 bots\r\n   Crawl delay: 5 sec\r\n  The path is scrapable for this user-agent\r\n\r\nDefine the path, where the news article of this website can be found:\r\n\r\n\r\nnews_path <- \"aktuelles\"\r\n\r\n\r\nDefine the CSS selector, which identifies all elements on the websites that\r\nare links to news articles:\r\n\r\n\r\narticles_css <- \".more-link\"\r\n\r\n\r\nWe now want to find all news articles on the website:\r\nModify the session path with ‘aktuelles’\r\nScrape the website\r\nLook for elements representing article links by searching for CSS selector\r\n‘.more-link’\r\n\r\n\r\nnews_links <- function(tsg_host, news_path, articles_css) {\r\n  host_news <- nod(tsg_host, path = news_path)\r\n\r\n  html <- scrape(host_news)\r\n\r\n  rows <- html |>\r\n    html_elements(articles_css)\r\n\r\n  rows |>\r\n    html_attr(\"href\") |>\r\n    map(\\(x) url_parse(x)) |>\r\n    map_chr(\"path\")\r\n}\r\n\r\n\r\n\r\n\r\npaths_news <- news_links(tsg_host, news_path, articles_css)\r\n\r\n\r\n\r\n\r\n\r\nIn total we have 418 articles to scrape.\r\nLook at some example paths:\r\n\r\n[1] \"/2022/04/04/unser-trainer-pascal-kopf-u16/\"                   \r\n[2] \"/2021/12/13/tombola-des-sparkassen-indoor-cups-2021/\"         \r\n[3] \"/2021/08/24/zum-tode-von-drago-todorovic/\"                    \r\n[4] \"/2021/09/20/regionalliga-klarer-41-sieg-in-grossaspach/\"      \r\n[5] \"/2023/09/10/regionalliga-63-spektakel-gegen-astoria-walldorf/\"\r\n\r\nWe want to extract the content of every article. We are looking for the\r\nfollowing parts of the post by searching for specific CSS expressions:\r\nTitle defined by ‘.gdlr-blog-title’\r\nLines defined by ‘.avia_textblock p’\r\n\r\n\r\nnews <- function(tsg_host, path_news, title_css, line_css) {\r\n  host_detail <- nod(tsg_host, path_news)\r\n  html_detail <- scrape(host_detail)\r\n  tibble(\r\n    title = html_element(html_detail, title_css) |> html_text2(),\r\n    line = html_elements(html_detail, line_css) |> html_text2(),\r\n    path = path_news)\r\n}\r\n\r\n\r\nApply the function for each path:\r\n\r\n\r\ndf_news <- map_df(paths_news, \\(x) news(tsg_host, x, title_css, line_css))\r\n\r\n\r\nApplying this function multiple times and obeying the scraping restriction at\r\nthe same time, can be quite time-consuming. Therefore, we defined in the\r\ntargets pipeline (take a look at ’_targets.R’), that the function is executed\r\nexactly once per article. This means future runs of the pipeline will detect\r\nif an article is already scraped and only scrape newly added articles, making\r\nfuture runs of the pipeline much faster.\r\nSometimes the content seems to be of solely technical nature. Define a regular\r\nexpression to search for these lines\r\n\r\n\r\ntech_regex <- \"xml\"\r\n\r\n\r\nWe now want to extract the words from the content we scraped. Before we do so\r\nwith the unnest_tokens function from the tidytext package, we exclude\r\nsome lines that have solely technical content, by searching for keyword\r\n‘xml’:\r\n\r\n\r\nwords_raw <- function(df_news, tech_regex) {\r\n  df_news |>\r\n    filter(str_detect(line, tech_regex, negate = TRUE)) |>\r\n    unnest_tokens(word, line)\r\n}\r\n\r\n\r\n\r\n\r\ndf_words_raw <- words_raw(df_news, tech_regex)\r\n\r\n\r\nBefore further analysis of the content, exclude some words that are not relevant\r\nfor this analysis:\r\nGerman stopwords\r\nEnglish stopwords\r\nWords that contain solely numeric characters\r\n\r\n\r\nwords <- function(df_words_raw) {\r\n  df_words_raw |>\r\n    anti_join(get_stopwords(language = \"de\"), by = join_by(word)) |>\r\n    anti_join(get_stopwords(language = \"en\"), by = join_by(word)) |>\r\n    filter(str_detect(word, \"^\\\\d+$\", negate = TRUE))\r\n}\r\n\r\n\r\n\r\n\r\ndf_words <- words(df_words_raw)\r\n\r\n\r\nAnalysis\r\nWe want to finish the analysis by creating a wordcloud of the scraped content.\r\nDefine the number of top words:\r\n\r\n\r\ntop_n_words <- 200L\r\n\r\n\r\nCount all words and filter for top 200.\r\n\r\n\r\nwords_count <- function(df_words, top_n_words) {\r\n  df_words |>\r\n    count(word, sort = TRUE) |>\r\n    top_n(top_n_words, wt = n)\r\n}\r\n\r\n\r\n\r\n\r\ndf_words_count <- words_count(df_words, top_n_words)\r\n\r\n\r\nCreate word cloud:\r\n\r\n\r\nvis_word_cloud <- function(df_words_count) {\r\n  df_words_count |>\r\n    ggplot() +\r\n    geom_text_wordcloud_area(aes(label = word, size = n)) +\r\n    scale_size_area(max_size = 50) +\r\n    theme_void()\r\n}\r\n\r\n\r\n\r\n\r\ngg_word_cloud <- vis_word_cloud(df_words_count)\r\n\r\n\r\n\r\n\r\n\r\nAnd there you go! A complete website scraped in a polite way and displayed with\r\na nice word cloud. Future updates of this analysis are quickly done, because\r\nonly new content is scraped, and old content is saved in the background.\r\nHappy times! Looking forward to further adventures using the techniques\r\nintroduced in this blog post.\r\n\r\n\r\n\r\n",
    "preview": "posts/friendly-webscraping/distill-preview.png",
    "last_modified": "2024-03-28T20:40:47+01:00",
    "input_file": {},
    "preview_width": 2100,
    "preview_height": 2100
  },
  {
    "path": "posts/scraping-tour-de-france-data/",
    "title": "Scraping Tour de France Data",
    "description": "An article about friendly webscraping and a remarkable Time Trial",
    "author": [
      {
        "name": "Julian During",
        "url": "https://example.com/norajones"
      }
    ],
    "date": "2024-02-14",
    "categories": [],
    "contents": "\r\nIdea\r\nIn Tour de France history, time trials always are a spectacle of their own.\r\nOftentimes they are a decisive factor on who wins the general classification\r\neach year. This was also the case this year with a stunning performance of\r\nJonas Vingegaard on the 16th stage from Passy to Combloux.\r\nIn this post we want to look at the performance of Jonas\r\nVingegaard in more detail, heavily inspired by this post\r\nhere.\r\nIn order to reproduce this analysis in one go,\r\nyou have to perform the following steps:\r\nClone the repository\r\nRun renv::restore()\r\nRun targets::tar_make()\r\nAlternatively you could copy and paste the code chunks into your R session\r\nand execute them one after the other.\r\nYou would have to install the following packages by hand:\r\n\r\n\r\nlibrary(tarchetypes)\r\nlibrary(ggbeeswarm)\r\nlibrary(conflicted)\r\nlibrary(tidyverse)\r\nlibrary(assertr)\r\nlibrary(distill)\r\nlibrary(janitor)\r\nlibrary(targets)\r\nlibrary(polite)\r\nlibrary(scales)\r\nlibrary(rvest)\r\nlibrary(fs)\r\n\r\nconflict_prefer(\"filter\", \"dplyr\")\r\n\r\n\r\nData\r\nDefine global variables:\r\n\r\n\r\ncycling_stats_url <- \"https://www.procyclingstats.com/\"\r\n\r\n\r\n\r\n\r\nstart_year <- 2000\r\n\r\n\r\n\r\n\r\nstages_overview_path <- \"race/tour-de-france/\"\r\n\r\n\r\n\r\n\r\nstages_urls_css <- \".basic a:nth-child(1)\"\r\n\r\n\r\n\r\n\r\ntime_trial_regex <- regex(\"\\\\(ITT\\\\)\", ignore_case = TRUE)\r\n\r\n\r\nFirst we need the raw data. We will get the raw data from\r\n’https://www.procyclingstats.com/.\r\nThe exact path where we will get the data, is determined by the string\r\n‘race/tour-de-france/’ + the year\r\n(up until year 2000) of the Tour de France edtion:\r\n\r\n\r\ntdf_editions <- function(start_year, stages_overview_path) {\r\n  years <- start_year:year(today())\r\n  \r\n  tibble(\r\n    year = start_year:year(today()),\r\n    so_path = str_glue(\"{stages_overview_path}{year}\"))\r\n}\r\n\r\n\r\n\r\n\r\ndf_tdf_editions <- tdf_editions(start_year, stages_overview_path)\r\n\r\n\r\n\r\n# A tibble: 25 × 2\r\n    year so_path                 \r\n   <int> <glue>                  \r\n 1  2000 race/tour-de-france/2000\r\n 2  2001 race/tour-de-france/2001\r\n 3  2002 race/tour-de-france/2002\r\n 4  2003 race/tour-de-france/2003\r\n 5  2004 race/tour-de-france/2004\r\n 6  2005 race/tour-de-france/2005\r\n 7  2006 race/tour-de-france/2006\r\n 8  2007 race/tour-de-france/2007\r\n 9  2008 race/tour-de-france/2008\r\n10  2009 race/tour-de-france/2009\r\n# ℹ 15 more rows\r\n\r\nPull all paths into a vector for later analysis:\r\n\r\n\r\nso_paths <- pull(df_tdf_editions, so_path)\r\n\r\n\r\nScrape the stage overview for each edition by using the following helper\r\nfunction.\r\nAfter a first introduction (next code chunk) we alter the url by adding the\r\npath from the above code chunk.\r\nWe do this by applying the polite::nod function.\r\nAfter the exact url is determined, we scrape the content of the webpage using\r\nthe polite::scrape function.\r\nKeep only certain elements of the scraped html by looking\r\nfor CSS ‘.basic a:nth-child(1)’. The CSS was determined using\r\ntechniques described in this vignette.\r\n\r\n\r\nscrape_overview <- function(session_bow, so_path, stages_urls_css) {\r\n  scrape_url <- nod(session_bow, path = so_path)\r\n  \r\n  stages_overview_html <- scrape(scrape_url)\r\n  \r\n  stages_overview_nodes <- stages_overview_html |>\r\n    html_elements(stages_urls_css)\r\n  \r\n  tibble(\r\n    href = html_attr(stages_overview_nodes, \"href\"),\r\n    desc = html_text(stages_overview_nodes)) |>\r\n    mutate(url = scrape_url$url)\r\n}\r\n\r\n\r\nBefore we apply this function multiple times, we first introduce ourselves with\r\nthe polite::bow function. After that we apply the above function to each\r\nedition. By doing this we automatically apply to the\r\nscraping restrictions defined in robots.txt.\r\n\r\n\r\nstages_overview_raw <- function(cycling_stats_url, so_paths, stages_urls_css) {\r\n  session_bow <- bow(cycling_stats_url)\r\n  \r\n  map_df(so_paths, \\(x) scrape_overview(session_bow, x, stages_urls_css))\r\n}\r\n\r\n\r\n\r\n\r\ndf_stages_overview_raw <- stages_overview_raw(cycling_stats_url, so_paths, stages_urls_css)\r\n\r\n\r\nThe result looks like this:\r\n\r\n# A tibble: 888 × 3\r\n   href                         desc                 url              \r\n   <chr>                        <chr>                <chr>            \r\n 1 team/us-postal-service-2000  US Postal Service    https://www.proc…\r\n 2 team/team-telekom-2000       Team Telekom         https://www.proc…\r\n 3 team/festina-lotus-2000      Festina - Lotus      https://www.proc…\r\n 4 team/festina-lotus-2000      Festina - Lotus      https://www.proc…\r\n 5 team/kelme-costa-blanca-2000 Kelme - Costa Blanca https://www.proc…\r\n 6 team/polti-2000              Polti                https://www.proc…\r\n 7 team/kelme-costa-blanca-2000 Kelme - Costa Blanca https://www.proc…\r\n 8 team/kelme-costa-blanca-2000 Kelme - Costa Blanca https://www.proc…\r\n 9 team/banesto-2000            Banesto              https://www.proc…\r\n10 team/mapei-quickstep-2000    Mapei - Quickstep    https://www.proc…\r\n# ℹ 878 more rows\r\n\r\nPreprocess stages overview. Only keep rows with description and extract year\r\nof the edition from href:\r\n\r\n\r\nstages_overview <- function(df_stages_overview_raw, cycling_stats_url) {\r\n  df_stages_overview_raw |>\r\n    filter(desc != \"\") |>\r\n    rename(url_stages_overview = url) |>\r\n    mutate(\r\n      year = parse_integer(\r\n        map_chr(str_split(url_stages_overview, \"/\"), \\(x) x[length(x)])),\r\n      url_stage = str_glue(\"{cycling_stats_url}{href}\")) |>\r\n    filter(year <= 2023)\r\n}\r\n\r\n\r\n\r\n\r\ndf_stages_overview <- stages_overview(df_stages_overview_raw, cycling_stats_url)\r\n\r\n\r\nFilter for time trial stages. Keep only rows containing the following key word\r\nto do so:\r\n\r\n[1] \"\\\\(ITT\\\\)\"\r\n\r\nAfter filtering, further process\r\nthe description to keep the string short and simple:\r\n\r\n\r\nstages_itt <- function(df_stages_overview, time_trial_regex) {\r\n  df_stages_overview |>\r\n    filter(str_detect(desc, time_trial_regex)) |>\r\n    distinct(href, .keep_all = TRUE) |>\r\n    mutate(\r\n      desc = str_remove_all(desc, str_glue(\"{time_trial_regex}|Stage|\\\\|\")),\r\n      desc = str_squish(desc),\r\n      desc = str_glue(\"{desc} ({year})\"))\r\n}\r\n\r\n\r\n\r\n\r\ndf_stages_itt <- stages_itt(df_stages_overview, time_trial_regex)\r\n\r\n\r\n\r\n# A tibble: 40 × 5\r\n   href                      desc  url_stages_overview  year url_stage\r\n   <chr>                     <glu> <chr>               <int> <glue>   \r\n 1 race/tour-de-france/2000… 1 Fu… https://www.procyc…  2000 https://…\r\n 2 race/tour-de-france/2000… 19 F… https://www.procyc…  2000 https://…\r\n 3 race/tour-de-france/2001… 11 G… https://www.procyc…  2001 https://…\r\n 4 race/tour-de-france/2001… 18 M… https://www.procyc…  2001 https://…\r\n 5 race/tour-de-france/2002… 9 La… https://www.procyc…  2002 https://…\r\n 6 race/tour-de-france/2002… 19 R… https://www.procyc…  2002 https://…\r\n 7 race/tour-de-france/2003… 12 G… https://www.procyc…  2003 https://…\r\n 8 race/tour-de-france/2003… 19 P… https://www.procyc…  2003 https://…\r\n 9 race/tour-de-france/2004… 16 B… https://www.procyc…  2004 https://…\r\n10 race/tour-de-france/2004… 19 B… https://www.procyc…  2004 https://…\r\n# ℹ 30 more rows\r\n\r\nPull all the links into a vector for later analysis:\r\n\r\n\r\nstages_paths <- pull(df_stages_itt, href)\r\n\r\n\r\nFor every time trial, scrape the result of the stage. Use the following helper\r\nfunction. In the scraped html look for the CSS defined by\r\n‘div.result-cont:nth-child(5) > div:nth-child(1) > table:nth-child(1)’.\r\n\r\n\r\nscrape_stage <- function(session_bow, stage_path, stage_tbl_css) {\r\n  stage_url <- nod(session_bow, path = stage_path)\r\n  \r\n  stage_html <- scrape(stage_url)\r\n  \r\n  html_nodes(stage_html, stage_tbl_css) |>\r\n    html_table() |>\r\n    first() |>\r\n    clean_names() |>\r\n    mutate(rnk = row_number(), url_stage = stage_url$url)\r\n}\r\n\r\n\r\nAgain, first bow to the host and then apply the function from the\r\nabove code chunk repeatedly.\r\n\r\n\r\nstage <- function(cycling_stats_url, stages_paths, stage_tbl_css) {\r\n  session_bow <- bow(cycling_stats_url)\r\n  \r\n  map_df(stages_paths, \\(x) scrape_stage(session_bow, x, stage_tbl_css))\r\n}\r\n\r\n\r\n\r\n\r\ndf_stage <- stage(cycling_stats_url, stages_paths, stage_tbl_css)\r\n\r\n\r\n\r\n# A tibble: 6,569 × 15\r\n     rnk    gc timelag   bib h2h   specialty rider     age team    uci\r\n   <int> <int> <chr>   <int> <lgl> <chr>     <chr>   <int> <chr> <int>\r\n 1     1     1 +0:00     127 NA    TT        MILLAR…    23 Cofi…    NA\r\n 2     2     2 +0:02       1 NA    TT        ARMSTR…    28 US P…    NA\r\n 3     3     3 +0:13      51 NA    Sprint    JALABE…    31 O.N.…    NA\r\n 4     4     4 +0:14      61 NA    TT        ULLRIC…    26 Team…    NA\r\n 5     5     5 +0:17      52 NA    GC        CAÑADA…    25 O.N.…    NA\r\n 6     6     6 +0:20      11 NA    TT        ZÜLLE …    31 Bane…    NA\r\n 7     7     7 +0:21       3 NA    TT        EKIMOV…    34 US P…    NA\r\n 8     8     8 +0:27      72 NA    Classic   BORGHE…    31 Merc…    NA\r\n 9     9     9 +0:33       4 NA    TT        HAMILT…    29 US P…    NA\r\n10    10    10 +0:36      43 NA    Classic   DEKKER…    29 Rabo…    NA\r\n# ℹ 6,559 more rows\r\n# ℹ 5 more variables: pnt <int>, x <lgl>, time <chr>, avg <dbl>,\r\n#   url_stage <chr>\r\n\r\nCalculate time delta to winner time for each rider:\r\n\r\n\r\ntime_delta <- function(df_stage) {\r\n  df_stage |>\r\n    transmute(\r\n      url_stage, rider,\r\n      time_delta = case_when(\r\n        rnk == 1 ~ seconds(0),\r\n        str_detect(time, \",\") ~ ms(str_remove_all(time, \",\")),\r\n        TRUE ~ ms(str_sub(time, start = (str_length(time) / 2) + 1)))) |>\r\n    filter(!is.na(time_delta), time_delta >= 0)\r\n}\r\n\r\n\r\n\r\n\r\ndf_time_delta <- time_delta(df_stage)\r\n\r\n\r\n\r\n# A tibble: 6,548 × 3\r\n   url_stage                                          rider time_delta\r\n   <chr>                                              <chr> <Period>  \r\n 1 https://www.procyclingstats.com/race/tour-de-fran… MILL… 0S        \r\n 2 https://www.procyclingstats.com/race/tour-de-fran… ARMS… 2S        \r\n 3 https://www.procyclingstats.com/race/tour-de-fran… JALA… 13S       \r\n 4 https://www.procyclingstats.com/race/tour-de-fran… ULLR… 14S       \r\n 5 https://www.procyclingstats.com/race/tour-de-fran… CAÑA… 16S       \r\n 6 https://www.procyclingstats.com/race/tour-de-fran… ZÜLL… 20S       \r\n 7 https://www.procyclingstats.com/race/tour-de-fran… EKIM… 21S       \r\n 8 https://www.procyclingstats.com/race/tour-de-fran… BORG… 27S       \r\n 9 https://www.procyclingstats.com/race/tour-de-fran… HAMI… 33S       \r\n10 https://www.procyclingstats.com/race/tour-de-fran… DEKK… 36S       \r\n# ℹ 6,538 more rows\r\n\r\nExtract time for each winner:\r\n\r\n\r\nwinner_time <- function(df_stage) {\r\n  df_stage |>\r\n    group_by(url_stage) |>\r\n    summarise(winner_time = time[rnk == 1]) |>\r\n    mutate(winner_time = hms(winner_time))\r\n}\r\n\r\n\r\n\r\n\r\ndf_winner_time <- winner_time(df_stage)\r\n\r\n\r\nAnalysis\r\nCombine everything into one data frame. Calculate total time for each rider:\r\n\r\n\r\ntotal_time <- function(df_time_delta, df_winner_time, df_stages_itt) {\r\n  df_time_delta |>\r\n    inner_join(df_winner_time, by = \"url_stage\") |>\r\n    transmute(\r\n      url_stage, rider,\r\n      time = winner_time + time_delta,\r\n      diff_winner_perc = (time - winner_time) / winner_time) |>\r\n    left_join(df_stages_itt, by = \"url_stage\")\r\n}\r\n\r\n\r\n\r\n\r\ndf_total_time <- total_time(df_time_delta, df_winner_time, df_stages_itt)\r\n\r\n\r\nSummarise data per stage. Use this summary to arrange the description as an\r\nordered factor. Exclude winner rows from the data:\r\n\r\n\r\ntotal_time_summary <- function(df_total_time) {\r\n  df_total_time_summary <- df_total_time |>\r\n    filter(diff_winner_perc != 0) |>\r\n    group_by(desc) |>\r\n    summarise(median_diff_winner_perc = median(diff_winner_perc)) |>\r\n    arrange(desc(median_diff_winner_perc)) |>\r\n    mutate(desc = fct_inorder(desc))\r\n  \r\n  df_total_time |>\r\n    filter(diff_winner_perc != 0) |>\r\n    mutate(desc = fct_rev(factor(desc, df_total_time_summary$desc)))\r\n}\r\n\r\n\r\n\r\n\r\ndf_total_time_summary <- total_time_summary(df_total_time)\r\n\r\n\r\nVisualize data. Arrange the stages based on the summary calculated in the\r\npreceding code chunk:\r\n\r\n\r\nvis_total_time <- function(df_total_time_summary) {\r\n  set.seed(10923)\r\n  \r\n  df_total_time_summary |>\r\n    ggplot(aes(y = desc, x = diff_winner_perc)) +\r\n    geom_beeswarm(alpha = 0.2) +\r\n    geom_boxplot() +\r\n    theme_light() +\r\n    theme(axis.text.x = element_text(angle = 90)) +\r\n    scale_x_continuous(labels = label_percent()) +\r\n    labs(y = \"Stage\", x = \"Difference to Winner Time [%]\")\r\n}\r\n\r\n\r\n\r\n\r\ngg_total_time <- vis_total_time(df_total_time_summary)\r\n\r\n\r\n\r\n\r\n\r\nThe top stages are almost all (except for Cap Découverte) mountain time\r\ntrials. One can see that this years time trial stands out from the rest\r\nnonetheless!\r\n\r\n\r\n\r\n",
    "preview": "https://raw.githubusercontent.com/duju211/tour_tt/master/total_time.png",
    "last_modified": "2024-02-14T19:56:26+01:00",
    "input_file": {}
  },
  {
    "path": "posts/hanukkah-of-data-revisited-2023-speedrun/",
    "title": "Hanukkah of Data Revisited (2023 / Speedrun)",
    "description": "Revisit of fun data challenge from last year",
    "author": [
      {
        "name": "Julian During",
        "url": "www.datannery.com"
      }
    ],
    "date": "2023-12-31",
    "categories": [],
    "contents": "\r\n\r\n\r\n\r\nTitle photo from Diana Polekhina on Unsplash.\r\nChallenge\r\n‘Hanukkah of Data’ is a data challenge\r\nwhere you have to solve 8 puzzles surrounding a fictional data set.\r\nI have already participated in\r\nlast year’s challenge,\r\nbut it was a lot of fun to revisit the old puzzles and rework some of my\r\nsolutions.\r\nEspecially the ‘speed-run’ challenge had some twists in it, therefore I will\r\ntalk about these puzzles in this blog post. The puzzles are mostly the same\r\nas in the normal version, but the data has some more difficult edge cases in it.\r\nTo solve the puzzles we use the following R libraries:\r\n\r\n\r\nlibrary(tidyverse)\r\nlibrary(assertr)\r\n\r\n\r\nI’ve already used the tidyverse in last years challenge. This year I am\r\nalso using assertr to detect problems with my solutions as early as possible.\r\nBehind the scenes the whole analysis is created as a targets pipeline.\r\nSo if you want to reproduce the analysis, you have to perform the following\r\nsteps:\r\nClone the repository\r\nRun renv::restore() to restore the R package versions\r\nRun targets::tar_make() to run the pipeline\r\nDay 1\r\nTo find the rug, we will need to contact a private investigator.\r\nThe last name of the investigator can be spelled by using the letters printed\r\non the phone buttons. For example: 2 has “ABC”, and 3 “DEF”, etc.\r\nThe key pad can be represented in R like this:\r\n\r\n\r\nphone_letter <- function() {\r\n  tibble(letter = letters) |>\r\n    mutate(\r\n      nr = as.character(case_when(\r\n        letter %in% c(\"a\", \"b\", \"c\") ~ 2,\r\n        letter %in% c(\"d\", \"e\", \"f\") ~ 3,\r\n        letter %in% c(\"g\", \"h\", \"i\") ~ 4,\r\n        letter %in% c(\"j\", \"k\", \"l\") ~ 5,\r\n        letter %in% c(\"m\", \"n\", \"o\") ~ 6,\r\n        letter %in% c(\"p\", \"q\", \"r\", \"s\") ~ 7,\r\n        letter %in% c(\"t\", \"u\", \"v\") ~ 8,\r\n        letter %in% c(\"w\", \"x\", \"y\", \"z\") ~ 9,\r\n        TRUE ~ NA_real_)))\r\n}\r\n\r\n\r\nWe then need to find the last name of each person.\r\nThis can be a little bit tricky. Names can also include special suffixes\r\nlike Jr. or roman numbers like III. Therefore we use a regex to filter for\r\nlast names that start with a upper case letter and end with one or more lower\r\ncase letters.\r\nAfter that we transform the data so that every letter is one row:\r\n\r\n\r\nlast_names <- function(df_customers) {\r\n  df_customers |>\r\n    transmute(\r\n      customerid,\r\n      name_split = str_split(name, \"\\\\s+\"),\r\n      name_split = map(\r\n        name_split,\r\n        ~ str_subset(.x, regex(\"^[A-Z][a-z]+$\"))),\r\n      last_name = map_chr(name_split, last),\r\n      letter = str_split(last_name, \"\"),\r\n      phone_chr = str_remove_all(phone, \"-\")) |>\r\n    unnest(letter) |>\r\n    mutate(letter = str_to_lower(letter))\r\n}\r\n\r\n\r\nBy combining both data sources, we can answer the question:\r\n\r\n\r\ninvestigator <- function(df_last_names, df_phone_letter) {\r\n  df_last_names |>\r\n    left_join(df_phone_letter, by = \"letter\") |>\r\n    group_by(customerid, phone_chr) |>\r\n    summarise(phone_pro = str_flatten(nr), .groups = \"drop\") |>\r\n    filter(phone_chr == phone_pro) |>\r\n    verify(length(customerid) == 1)\r\n}\r\n\r\n\r\n\r\n\r\n  \r\n  customerid\r\n      name\r\n      phone\r\n    4249\r\nTracy Rosenkranz\r\n767-365-7269\r\n\r\nDay 2\r\nNow we are looking for a contractor, to whom the rug was given to by a\r\ncleaning company.\r\nLook for customers with the searched initials\r\n(‘ds’, for the speed run):\r\n\r\n\r\ninitials <- function(df_customers, searched_initials) {\r\n  df_customers |>\r\n    mutate(\r\n      name_split = str_split(name, \"\\\\s+\"),\r\n      name_split = map(\r\n        name_split, ~ str_subset(.x, regex(\"^[a-z]+$\", ignore_case = TRUE))),\r\n      first_name = map_chr(name_split, first),\r\n      last_name = map_chr(name_split, last)) |>\r\n    transmute(\r\n      customerid,\r\n      initials = str_to_lower(str_c(\r\n        str_sub(first_name, end = 1), str_sub(last_name, end = 1)))) |>\r\n    filter(initials == searched_initials)\r\n}\r\n\r\n\r\nWe then look for products that are ‘coffee’ or ‘bagels’.\r\nJoin the order items to the filtered products:\r\n\r\n\r\ncoffee_bagels <- function(df_products, df_order_items) {\r\n  df_coffee <- df_products |>\r\n    filter(str_detect(desc, regex(\"coffee\", ignore_case = TRUE)))\r\n  \r\n  df_bagel <- df_products |>\r\n    filter(str_detect(desc, regex(\"bagel\", ignore_case = TRUE)))\r\n  \r\n  bind_rows(\r\n    list(bagel = df_bagel, coffee = df_coffee), .id = \"coffee_bagel\") |>\r\n    left_join(df_order_items, by = \"sku\")\r\n}\r\n\r\n\r\nLook for 2017 orders where coffee or bagels were bought.\r\nKeep only those were the customer has the above mentioned initials.\r\n\r\n\r\norder_contractor <- function(df_orders, df_coffee_bagels, df_initials) {\r\n  df_orders |>\r\n    filter(year(ordered) == 2017) |>\r\n    inner_join(df_coffee_bagels, by = \"orderid\") |>\r\n    group_by(customerid, day = floor_date(ordered, unit = \"day\")) |>\r\n    summarise(\r\n      coffee = any(coffee_bagel == \"coffee\"),\r\n      bagel = any(coffee_bagel == \"bagel\"), .groups = \"drop_last\") |>\r\n    summarise(coffee_and_bagel = any(coffee & bagel)) |>\r\n    filter(coffee_and_bagel) |>\r\n    semi_join(df_initials, by = \"customerid\") |>\r\n    verify(length(customerid) == 1)\r\n}\r\n\r\n\r\n\r\n\r\n  \r\n  customerid\r\n      name\r\n      phone\r\n    5745\r\nDavid Swanson Jr.\r\n838-351-0370\r\n\r\nDay 3\r\nSearch for the neighbor with the spider hat. The filtering conditions are the\r\nfollowing:\r\nBorn in goat year\r\nBorn in libra zodiac\r\nLives in neighborhood to contractor\r\nThese are the specific filter conditions for the speed run. For the other\r\nversions of the game, these are different. But the general filtering stays the\r\nsame.\r\nDefine goat years (pasted from wikipedia):\r\n\r\n\r\ngoat_years <- function() {\r\n  df_goat_raw <- tibble::tribble(\r\n           ~Start.date,          ~End.date, ~Heavenly.branch,\r\n    \"13 February 1907\",  \"1 February 1908\",      \"Fire Goat\",\r\n     \"1 February 1919\", \"19 February 1920\",     \"Earth Goat\",\r\n    \"17 February 1931\",  \"5 February 1932\",     \"Metal Goat\",\r\n     \"5 February 1943\",  \"24 January 1944\",     \"Water Goat\",\r\n     \"24 January 1955\", \"11 February 1956\",      \"Wood Goat\",\r\n     \"9 February 1967\",  \"29 January 1968\",      \"Fire Goat\",\r\n     \"28 January 1979\", \"15 February 1980\",     \"Earth Goat\",\r\n    \"15 February 1991\",  \"3 February 1992\",     \"Metal Goat\",\r\n     \"1 February 2003\",  \"21 January 2004\",     \"Water Goat\",\r\n    \"19 February 2015\",  \"7 February 2016\",      \"Wood Goat\",\r\n     \"6 February 2027\",  \"25 January 2028\",      \"Fire Goat\",\r\n     \"24 January 2039\", \"11 February 2040\",     \"Earth Goat\",\r\n    \"11 February 2051\",  \"31 January 2052\",     \"Metal Goat\",\r\n     \"29 January 2063\", \"16 February 2064\",     \"Water Goat\",\r\n    \"15 February 2075\",  \"4 February 2076\",      \"Wood Goat\",\r\n     \"3 February 2087\",  \"23 January 2088\",      \"Fire Goat\",\r\n     \"21 January 2099\",  \"8 February 2100\",     \"Earth Goat\") |>\r\n    clean_names()\r\n  \r\n  df_goat_raw |>\r\n    mutate(\r\n      across(c(start_date, end_date), ~ parse_date(.x, \"%d %B %Y\")))\r\n}\r\n\r\n\r\n\r\n\r\nspider_hat <- function(df_customers, df_contractor, df_chinese_year) {\r\n  df_customers |>\r\n    filter(\r\n      map_lgl(\r\n        birthdate,\r\n        ~ any(\r\n          df_chinese_year$start_date <= .x & df_chinese_year$end_date >= .x)),\r\n      case_when(\r\n        month(birthdate) == 9 ~ day(birthdate) >= 23,\r\n        month(birthdate) == 10 ~ day(birthdate) <= 23,\r\n        TRUE ~ FALSE),\r\n      str_detect(citystatezip, df_contractor$citystatezip)) |>\r\n    select(customerid, name, citystatezip, phone) |>\r\n    verify(length(customerid) == 1)\r\n}\r\n\r\n\r\n\r\n\r\n  \r\n  customerid\r\n      name\r\n      citystatezip\r\n      phone\r\n    3864\r\nDavid Perez\r\nQueens Village, NY 11427\r\n914-594-5535\r\n\r\nDay 4\r\nLook for order items that are ‘pastries’:\r\n\r\n\r\norder_items_pastries <- function(df_order_items, df_products) {\r\n  df_products_pastries <- df_products |>\r\n    filter(str_detect(sku, regex(\"bky\", ignore_case = TRUE)))\r\n  \r\n  df_order_items |>\r\n    semi_join(df_products_pastries, by = \"sku\")\r\n}\r\n\r\n\r\nLook for persons that order pastries early in the morning:\r\n\r\n\r\ntinder_woman <- function(df_orders, df_order_items_pastries, df_customers) {\r\n  df_order_items_pastries |>\r\n    left_join(df_orders, by = \"orderid\") |>\r\n    filter(hour(ordered) < 9) |>\r\n    arrange(ordered) |>\r\n    group_by(day = floor_date(ordered, \"day\")) |>\r\n    summarise(\r\n      earliest_order = min(ordered),\r\n      customerid = unique(customerid[ordered == earliest_order])) |>\r\n    count(customerid, sort = TRUE) |>\r\n    slice(1) |>\r\n    left_join(\r\n      select(df_customers, customerid, name, phone),\r\n      by = c(\"customerid\"))\r\n}\r\n\r\n\r\n\r\n\r\n  \r\n  customerid\r\n      n\r\n      name\r\n      phone\r\n    6455\r\n13\r\nBrittany Harmon\r\n716-789-4433\r\n\r\nDay 5\r\nLook for people that live in Staten Island (not needed for the speedrun):\r\n\r\n\r\nstaten_island <- function(df_customers) {\r\n  df_customers |>\r\n    transmute(customerid, city = map_chr(str_split(citystatezip, \",\"), 1)) |>\r\n    filter(str_detect(city, regex(\"Staten\\\\s+Island\", ignore_case = TRUE)))\r\n}\r\n\r\n\r\nLook for products that represent cat food for senior cats:\r\n\r\n\r\nsenior_cat_food <- function(df_products) {\r\n  df_products |>\r\n    filter(\r\n      str_detect(desc, regex(\"cat\\\\s+food\", ignore_case = TRUE)),\r\n      str_detect(desc, regex(\"senior\", ignore_case = TRUE)))\r\n}\r\n\r\n\r\nCombine the information and look for the searched woman:\r\n\r\n\r\ncat_lady <- function(df_order_items, df_orders, df_senior_cat_food,\r\n                     df_staten_island) {\r\n  df_order_items |>\r\n    semi_join(df_senior_cat_food, by = \"sku\") |>\r\n    left_join(select(df_orders, orderid, customerid), by = \"orderid\") |>\r\n    #semi_join(df_staten_island, by = \"customerid\") |>\r\n    count(customerid, sort = TRUE) |>\r\n    slice(1)\r\n}\r\n\r\n\r\n\r\n\r\n  \r\n  customerid\r\n      name\r\n      citystatezip\r\n      phone\r\n    7957\r\nTheresa Carter\r\nBronx, NY 10461\r\n347-835-2358\r\n\r\nDay 6\r\nCalculate margin for each order item\r\n\r\n\r\norder_items_margin <- function(df_order_items, df_products) {\r\n  df_order_items |>\r\n    left_join(df_products, by = \"sku\") |>\r\n    group_by(orderid) |>\r\n    summarise(margin = sum(unit_price - wholesale_cost))\r\n}\r\n\r\n\r\nDetermine customer with the lowest total margin:\r\n\r\n\r\nfrugal_cousin <- function(df_orders, df_order_items_margin) {\r\n  df_orders |>\r\n    left_join(df_order_items_margin, by = \"orderid\") |>\r\n    group_by(customerid) |>\r\n    summarise(customer_margin = mean(margin)) |>\r\n    arrange(customer_margin) |>\r\n    slice(1)\r\n}\r\n\r\n\r\n\r\n\r\n  \r\n  customerid\r\n      name\r\n      phone\r\n    8884\r\nDeborah Green\r\n838-295-7143\r\n\r\nDay 7\r\nFind all orders that contain a colored item:\r\n\r\n\r\ncolor_orders <- function(df_orders, df_order_items, df_products) {\r\n  df_orders |>\r\n    left_join(df_order_items, by = c(\"orderid\")) |>\r\n    left_join(df_products, by = \"sku\") |>\r\n    mutate(\r\n      color = str_remove_all(str_extract(desc, \"\\\\(.+\\\\)\"), \"\\\\(|\\\\)\"),\r\n      day = as_date(floor_date(ordered, unit = \"day\"))) |>\r\n    filter(!is.na(color))\r\n}\r\n\r\n\r\nSearch for orders that happened in close proximity to the orders of the\r\nfrugal cousin:\r\n\r\n\r\nex_boyfriend <- function(df_color_orders, df_frugal_cousin) {\r\n  df_color_orders_fc <- df_color_orders |>\r\n    semi_join(df_frugal_cousin, by = \"customerid\") |>\r\n    mutate(start = ordered - dminutes(0.3), end = ordered + dminutes(0.3))\r\n  \r\n  df_color_orders |>\r\n    anti_join(df_color_orders_fc, by = join_by(customerid)) |>\r\n    inner_join(\r\n      select(df_color_orders_fc, day, start, end), by = join_by(day)) |>\r\n    filter(ordered >= start & ordered <= end) |>\r\n    verify(length(customerid) == 1)\r\n}\r\n\r\n\r\n\r\n\r\n  \r\n  customerid\r\n      name\r\n      phone\r\n    9931\r\nJeremy Burch\r\n516-544-4187\r\n\r\nDay 8\r\nLook for products that are collectibles\r\n\r\n\r\ncollectibles <- function(df_products) {\r\n  df_products |>\r\n    filter(str_detect(sku, \"COL\"))\r\n}\r\n\r\n\r\nFind the person who has all the collectibles\r\n\r\n\r\ncollector <- function(df_orders, df_order_items, df_collectibles) {\r\n  df_order_items |>\r\n    semi_join(df_collectibles, by = \"sku\") |>\r\n    left_join(df_orders, by = \"orderid\") |>\r\n    group_by(customerid) |>\r\n    summarise(anz_coll = n_distinct(sku)) |>\r\n    filter(anz_coll == nrow(df_collectibles)) |>\r\n    verify(length(customerid) == 1)\r\n}\r\n\r\n\r\n\r\n\r\n  \r\n  customerid\r\n      name\r\n      phone\r\n    2602\r\nDaniel Wilson\r\n516-638-9966\r\n\r\nConclusion\r\nAs last year, I had a lot of fun solving the Hanukkah of Data challenges.\r\nI revisited my previous solutions and improved them to solve the new challenges.\r\nBy using functions from the assertr package, I could spot difficulties early.\r\nEspecially during the speed run at the end of the challenge, this type of\r\nassertive programming made it more easy for me, to adjust my solutions to more\r\nchallenging data and edge cases. I’m already looking forward to the challenges\r\nnext year :-).\r\n\r\n\r\n\r\n",
    "preview": "https://raw.githubusercontent.com/duju211/hanukkah_of_data/master/diana-polekhina-F6tbedzUQvw-unsplash.jpg",
    "last_modified": "2023-12-31T20:55:52+01:00",
    "input_file": {}
  },
  {
    "path": "posts/hanukkah-of-data/",
    "title": "Hanukkah of Data",
    "description": "Short data challenge released over 8 days of Hanukkah in 2022",
    "author": [
      {
        "name": "Julian During",
        "url": "www.datannery.com"
      }
    ],
    "date": "2023-01-15",
    "categories": [],
    "contents": "\r\n\r\n\r\n\r\nTitle photo from Gaelle Marcel on Unsplash\r\nChallenge\r\n‘Hanukkah of Data’ is a data challenge where you have to solve 8 puzzles surrounding a fictional data set.\r\nWe are asked to find a rug for our granduncle Noah who owns a store. The store has data about customer, products and orders. With the help of the data (CSV files) we are asked to solve a total of eight puzzles.\r\nData\r\nThe data for this analysis consists of four CSV files: ‘noahs-customers.csv’, ‘noahs-orders.csv’, ‘noahs-orders_items.csv’ and ‘noahs-products.csv’. The data is read into R with the help of the readr::read_csv function, specifying the correct column types as arguments in the function calls.\r\nCustomers\r\nThere are a total of 11080 customers in the raw data. Perform the following preprocessing steps:\r\nDetermine the Initials by splitting the name into first and last name\r\nExtract all digits from the phone number and save in column phone_chr\r\nRemove duplicates\r\n\r\ncustomers <- function(df_customers_raw) {\r\n  df_customers_raw |>\r\n    mutate(\r\n      phone_chr = map_chr(str_extract_all(phone, \"\\\\d\"), str_flatten),\r\n      name_split = str_split(name, \"\\\\s+\"),\r\n      first_name = map_chr(name_split, ~ str_flatten(.x[.x != last(.x)])),\r\n      last_name = map_chr(name_split, last),\r\n      initials = str_to_lower(str_glue(\r\n        \"{str_sub(first_name, end = 1)}{str_sub(last_name, end = 1)}\"))) |>\r\n    select(where(negate(is_list))) |>\r\n    distinct(customerid, .keep_all = TRUE)\r\n}\r\n\r\n\r\n# A tibble: 11,079 × 10\r\n   customerid name     address citystatezip birthdate  phone phone_chr\r\n   <chr>      <chr>    <chr>   <chr>        <date>     <chr> <chr>    \r\n 1 1001       Jack Qu… 201 E … Los Angeles… 1960-05-14 805-… 80528785…\r\n 2 1002       David P… 224C T… Staten Isla… 1978-04-04 516-… 51676816…\r\n 3 1003       Carrie … 1608 W… Tampa, FL 3… 1969-01-21 727-… 72720904…\r\n 4 1004       Steven … 178½ E… Manhattan, … 1953-08-17 607-… 60794195…\r\n 5 1005       Christi… 270 W … Bronx, NY 1… 1983-06-06 212-… 21275990…\r\n 6 1006       Amanda … 183-48… Saint Alban… 1962-07-08 914-… 91442131…\r\n 7 1007       Mark Co… 14-47 … College Poi… 1967-04-14 585-… 58555419…\r\n 8 1008       Jill St… 735A A… Manhattan, … 1959-06-11 516-… 51630704…\r\n 9 1009       Samuel … 56 Ric… Brooklyn, N… 1988-10-24 929-… 92986987…\r\n10 1010       Brenda … 2821 B… Bronx, NY 1… 1960-09-07 914-… 91420526…\r\n# ℹ 11,069 more rows\r\n# ℹ 3 more variables: first_name <chr>, last_name <chr>,\r\n#   initials <chr>\r\n\r\nProducts\r\nThere are a total of 1124 products. Perform the following preprocessing:\r\nExtract the category by removing all digits from the desc string\r\nExtract additional information that can be found in the bracket text\r\nRemove remaining brackets from add_info and desc\r\n\r\nproducts <- function(df_products_raw) {\r\n  df_products_raw |>\r\n    mutate(\r\n      category = str_remove(sku, \"\\\\d+\"),\r\n      add_info = str_extract(desc, \"\\\\(.+\\\\)\"),\r\n      desc = if_else(!is.na(add_info), str_remove(desc, add_info), desc),\r\n      across(c(add_info, desc), ~ str_trim(str_remove_all(.x, \"\\\\)|\\\\(\"))))\r\n}\r\n\r\n\r\n# A tibble: 1,124 × 5\r\n   sku     desc                       wholesale_cost category add_info\r\n   <chr>   <chr>                               <dbl> <chr>    <chr>   \r\n 1 DLI0002 Smoked Whitefish Sandwich            9.33 DLI      <NA>    \r\n 2 PET0005 Vegan Cat Food, Turkey & …           4.35 PET      <NA>    \r\n 3 HOM0018 Power Radio                         21.8  HOM      red     \r\n 4 KIT0034 Azure Ladle                          2.81 KIT      <NA>    \r\n 5 PET0041 Gluten-free Cat Food, Pum…           4.6  PET      <NA>    \r\n 6 PET0045 Gluten-free Cat Food, Sal…           4.32 PET      <NA>    \r\n 7 TOY0048 Electric Doll                       10.2  TOY      <NA>    \r\n 8 CMP0061 Network Printer                    136.   CMP      <NA>    \r\n 9 DLI0066 Pickled Herring Sandwich             9.94 DLI      <NA>    \r\n10 TOY0085 Noah's Toy Soldier                  12.0  TOY      <NA>    \r\n# ℹ 1,114 more rows\r\n\r\nOrders\r\nThere are a total of 214207 orders with\r\n427258 items ordered. Join the ordered item as\r\nlist column to make them easily accessible.\r\n\r\norders <- function(df_orders_raw, df_order_items) {\r\n  df_orders_raw |>\r\n    select(-items) |>\r\n    nest_join(df_order_items, by = \"orderid\", name = \"items\")\r\n}\r\n\r\nPuzzles\r\nPuzzle 1\r\nTo find the rug, we will need to contact a private investigator. The last name of the investigator can be spelled by using the letters printed on the phone buttons. For example: 2 has “ABC”, and 3 “DEF”, etc.\r\nThe key pad of the phone can be represented in R like this:\r\n\r\nphone_letter <- function() {\r\n  tibble(letter = letters) |>\r\n    mutate(\r\n      nr = ((row_number() - 1) %/% 3) + 2,\r\n      nr = as.character(if_else(nr == 10, 0, nr)))\r\n}\r\n\r\n\r\n# A tibble: 26 × 2\r\n   letter nr   \r\n   <chr>  <chr>\r\n 1 a      2    \r\n 2 b      2    \r\n 3 c      2    \r\n 4 d      3    \r\n 5 e      3    \r\n 6 f      3    \r\n 7 g      4    \r\n 8 h      4    \r\n 9 i      4    \r\n10 j      5    \r\n# ℹ 16 more rows\r\n\r\nBy combining this representation with the data on hand, we can determine the investigator:\r\n\r\ninvestigator <- function(df_customers, df_phone_letter) {\r\n  df_customers_pro <- df_customers |>\r\n    transmute(\r\n      name_pro = str_split(str_to_lower(str_remove_all(\r\n        last_name, \"\\\\s\"), \"\\\\s\"), \"\"),\r\n      customerid) |>\r\n    unnest(name_pro) |>\r\n    left_join(df_phone_letter, by = c(\"name_pro\" = \"letter\")) |>\r\n    group_by(customerid) |>\r\n    summarise(phone_pro = str_flatten(nr))\r\n  \r\n  df_customers |>\r\n    left_join(df_customers_pro, by = \"customerid\") |>\r\n    filter(str_length(str_extract(phone_chr, phone_pro)) == str_length(phone_chr))\r\n}\r\n\r\n\r\n\r\n  \r\n  customerid\r\n      name\r\n      phone\r\n    3188\r\nSam Guttenberg\r\n488-836-2374\r\n\r\nPuzzle 2\r\nNow we are looking for a contractor, to whom the rug was given to by a cleaning company. The following is known:\r\nThe contractor has the initials ‘jd’\r\nThe cleaning company and the contractor had meetings at Noah’s over coffee and bagels\r\nThe cleaning company stopped outsourcing a few years ago\r\n\r\ncontractor <- function(df_orders, df_customers, coffee_bagel_regex) {\r\n  df_customers_after_2017 <- df_orders |>\r\n    filter(year(ordered) > 2017) |>\r\n    distinct(customerid)\r\n  \r\n  df_rel_orders <- df_orders |>\r\n    filter(year(ordered) <= 2017) |>\r\n    anti_join(df_customers_after_2017, by = \"customerid\") |>\r\n    unnest(items) |>\r\n    filter(str_detect(desc, coffee_bagel_regex)) |>\r\n    semi_join(filter(df_customers, initials == \"jd\"), by = \"customerid\")\r\n  \r\n  df_customers |>\r\n    semi_join(df_rel_orders, by = \"customerid\")\r\n}\r\n\r\n\r\n\r\n  \r\n  customerid\r\n      name\r\n      citystatezip\r\n      phone\r\n    4164\r\nJeremy Davis\r\nSouth Ozone Park, NY 11420\r\n212-771-8924\r\n\r\nPuzzle 3\r\nWe are searching for the neighbor of the contractor. The neighbor has the following characteristics:\r\nStar Sign: Aries\r\nMarch 21 - April 19\r\n\r\nBorn in the year of the Dog\r\nEvery 12 years (2018 for example)\r\n\r\nLives in the neighborhood\r\nDog years are determined by the following function:\r\n\r\ndet_dog_years <- function() {\r\n  2018 - (1:9 * 12)\r\n}\r\n\r\nResulting in: 2006, 1994, 1982, 1970, 1958, 1946, 1934, 1922, 1910\r\nWith this information we can find out who the neighbor is:\r\n\r\nspider_hat <- function(df_customers, df_contractor, dog_years) {\r\n  df_customers |>\r\n    filter(year(birthdate) %in% dog_years) |>\r\n    filter(\r\n      case_when(\r\n        month(birthdate) == 3 ~ day(birthdate) >= 21,\r\n        month(birthdate) == 4 ~ day(birthdate) <= 20,\r\n        TRUE ~ FALSE)) |>\r\n    filter(str_detect(citystatezip, df_contractor$citystatezip))\r\n}\r\n\r\n\r\n\r\n  \r\n  customerid\r\n      name\r\n      citystatezip\r\n      birthdate\r\n      phone\r\n    2274\r\nBrent Nguyen\r\nSouth Ozone Park, NY 11420\r\n1958-03-25\r\n516-636-7397\r\n\r\nPuzzle 4\r\nThe next persons has the following habit:\r\nClaims the first pastries of the day\r\nFilter products for the ‘BKY’ category and search for the person which always orders these products first:\r\n\r\ntinder_woman <- function(df_products, df_orders, df_customers,\r\n                         product_pastries) {\r\n  df_products_pastries <- df_products |>\r\n    filter(category == product_pastries)\r\n  \r\n  df_orders_pastries <- df_orders |>\r\n    unnest(items) |>\r\n    semi_join(df_products_pastries, by = \"sku\")\r\n  \r\n  df_first_pastries <- df_orders_pastries |>\r\n    mutate(day = floor_date(ordered, unit = \"day\")) |>\r\n    group_by(day) |>\r\n    filter(ordered == min(ordered)) |>\r\n    ungroup() |>\r\n    count(customerid, sort = TRUE)\r\n  \r\n  df_customers |>\r\n    semi_join(slice(df_first_pastries, 1), by = \"customerid\")\r\n}\r\n\r\n\r\n\r\n  \r\n  customerid\r\n      name\r\n      phone\r\n    5375\r\nChristina Booker\r\n718-649-9036\r\n\r\nPuzzle 5\r\nIn the 5th puzzle we need to find a lady with a lot of cats as pets. We know about her and her pets:\r\nThe cats are senior\r\nShe lives in Queens\r\nShe has a lot of cats\r\n\r\ncat_lady <- function(df_products, df_orders, df_customers) {\r\n  df_products_cat <- df_products |>\r\n    filter(str_detect(desc, regex(\"cat\", ignore_case = TRUE))) |>\r\n    filter(str_detect(desc, regex(\"senior\", ignore_case = TRUE)))\r\n  \r\n  df_customers_queens <- df_customers |>\r\n    filter(str_detect(citystatezip, regex(\"queens\", ignore_case = TRUE)))\r\n  \r\n  df_customer_id <- df_orders |>\r\n    semi_join(df_customers_queens, by = \"customerid\") |>\r\n    unnest(items) |>\r\n    semi_join(df_products_cat, by = \"sku\") |>\r\n    group_by(customerid) |>\r\n    summarise(sku = str_flatten(unique(sku), \", \"), anz = n()) |>\r\n    filter(anz > 2)\r\n  \r\n  df_customers |>\r\n    semi_join(df_customer_id, by = \"customerid\")\r\n}\r\n\r\n\r\n\r\n  \r\n  customerid\r\n      name\r\n      phone\r\n    7675\r\nAnita Koch\r\n315-492-7411\r\n\r\nPuzzle 6\r\nThe cat lady has a very frugal cousin. She tells us: ” Noah […] loses money whenever she comes in the store”:\r\n\r\nfrugal_cousin <- function(df_orders, df_customers) {\r\n  df_customer_id <- df_orders |>\r\n    mutate(\r\n      profit = map_dbl(items, ~ sum(.x$unit_price - .x$wholesale_cost))) |>\r\n    group_by(customerid) |>\r\n    summarise(profit = sum(profit)) |>\r\n    top_n(n = 1, wt = -profit)\r\n  \r\n  df_customers |>\r\n    semi_join(df_customer_id, \"customerid\")\r\n}\r\n\r\n\r\n\r\n  \r\n  customerid\r\n      name\r\n      phone\r\n    8342\r\nEmily Randolph\r\n914-868-0316\r\n\r\nPuzzle 7\r\nThe frugal cousin tells us how she met her ex-boyfriend. They met while they were buying the same product, but in different colors. So we look at all the orders from the frugal cousin and search for that incident:\r\n\r\nex_boyfriend <- function(df_frugal_cousin, df_orders, df_customers) {\r\n  df_orders_color <- df_orders |>\r\n    semi_join(df_frugal_cousin, by = \"customerid\") |>\r\n    unnest(items) |>\r\n    mutate(day = floor_date(ordered, \"day\")) |>\r\n    filter(!is.na(add_info))\r\n  \r\n  df_order_rel <- df_orders |>\r\n    mutate(day = floor_date(ordered, \"day\")) |>\r\n    semi_join(df_orders_color, by = \"day\") |>\r\n    unnest(items) |>\r\n    inner_join(\r\n      df_orders_color, by = c(\"desc\", \"day\"),\r\n      suffix = c(\"_male\", \"_female\")) |>\r\n    filter(\r\n      add_info_male != add_info_female,\r\n      ordered_male >= ordered_female - dminutes(10)\r\n      & ordered_male <= ordered_female + dminutes(10))\r\n  \r\n  df_customers |>\r\n    semi_join(df_order_rel, by = c(\"customerid\" = \"customerid_male\"))\r\n}\r\n\r\n\r\n\r\n  \r\n  customerid\r\n      name\r\n      phone\r\n    8835\r\nJonathan Adams\r\n315-618-5263\r\n\r\nPuzzle 8\r\nFinally, we search for a collector. The collector is in possession of a complete set of Noah’s collectibles. By looking at the data, the relevant category is ‘COL’:\r\n\r\ncollector <- function(df_products, df_orders, df_customers, product_collect) {\r\n  df_products_collect <- df_products |>\r\n    filter(category == product_collect)\r\n  \r\n  df_orders_unnested <- df_orders |>\r\n    unnest(items)\r\n  \r\n  df_customer_id <- df_orders_unnested |>\r\n    semi_join(df_products_collect, by = c(\"sku\")) |>\r\n    group_by(customerid) |>\r\n    summarise(anz_col = n_distinct(sku)) |>\r\n    arrange(desc(anz_col)) |>\r\n    slice(1)\r\n  \r\n  df_customers |>\r\n    semi_join(df_customer_id, by = \"customerid\")\r\n}\r\n\r\n\r\n\r\n  \r\n  customerid\r\n      name\r\n      phone\r\n    4308\r\nTravis Bartlett\r\n929-906-5980\r\n\r\nEnd\r\nParticipating in the “Hanukkah of Data” competition was both challenging and\r\nrewarding. We successfully solved 8 difficult puzzles,\r\nsharpening our problem-solving and coding skills.\r\nI look forward to the challenge again next year and am eager to\r\nsee what it holds.\r\n\r\n\r\n\r\n",
    "preview": "https://github.com/duju211/hanukkah_of_data/blob/master/hanukkah.png?raw=true",
    "last_modified": "2023-12-01T22:56:56+01:00",
    "input_file": {}
  },
  {
    "path": "posts/hour-record-pacing-strategies/",
    "title": "Hour Record Pacing Strategies",
    "description": "A Comparison of Hour Record Pacing Strategies",
    "author": [
      {
        "name": "Julian During",
        "url": "https://www.datannery.com/"
      }
    ],
    "date": "2022-11-20",
    "categories": [],
    "contents": "\r\nIdea\r\nThe Hour Record is a attempt to drive a bike as far as possible in one hour.\r\nGet the data from two different hour record attempts and compare the different\r\npacing strategies in the following post. If you want to reproduce this analysis,\r\nyou have to perform the following steps:\r\nClone the repository from here\r\nInstall the packages listed in libraries.R\r\nRun targets::tar_make()\r\nData\r\nThe Data consists of screenshots of the lap numbers and times.\r\nExample image lap number:\r\n\r\n\r\n\r\nExample image lap time:\r\n\r\n\r\n\r\nGet lap number from image:\r\n\r\nnr_from_image <- function(img_path, bw_threshold) {\r\n  image_read(img_path) |> \r\n    image_quantize(colorspace = \"gray\") |>\r\n    image_threshold(threshold = bw_threshold) |>\r\n    ocr()\r\n}\r\n\r\nParse numbers:\r\n\r\ntime <- function(df_time_raw) {\r\n  df_time_raw |>\r\n    mutate(\r\n      across(where(is.character), parse_number),\r\n      ind = row_number())\r\n}\r\n\r\nPreprocess data:\r\nNice athlete name\r\nSpeed in kilometers per hour\r\n\r\ntime_pro <- function(df_time, df_athlete_dir) {\r\n  df_time |>\r\n    mutate(\r\n      athlete = str_remove(athlete, \"^df_time_\"),\r\n      speed = 0.25 / (time / 60 / 60)) |>\r\n    left_join(select(df_athlete_dir, athlete, distance), by = \"athlete\")\r\n}\r\n\r\n\r\n# A tibble: 444 × 6\r\n   athlete      time lap_nr   ind speed distance\r\n   <chr>       <dbl>  <dbl> <int> <dbl>    <dbl>\r\n 1 campenaerts  24.5      1     1  36.7     55.1\r\n 2 campenaerts  16.7      2     2  53.8     55.1\r\n 3 campenaerts  16.1      3     3  56.0     55.1\r\n 4 campenaerts  16.0      4     4  56.2     55.1\r\n 5 campenaerts  16.2      5     5  55.5     55.1\r\n 6 campenaerts  16.3      6     6  55.2     55.1\r\n 7 campenaerts  16.3      7     7  55.1     55.1\r\n 8 campenaerts  16.3      8     8  55.1     55.1\r\n 9 campenaerts  16.3      9     9  55.2     55.1\r\n10 campenaerts  16.3     10    10  55.3     55.1\r\n# … with 434 more rows\r\n\r\nYou can find the preprocessed data as CSV file here.\r\nPlot\r\n\r\nvis_time_all <- function(df_time_pro) {\r\n  df_time_pro |>\r\n    filter(lap_nr != 1) |>\r\n    mutate(athlete = str_to_title(athlete)) |>\r\n    ggplot(aes(x = lap_nr, y = speed, color = athlete)) +\r\n    geom_point() +\r\n    geom_hline(aes(yintercept = distance, color = athlete), linetype = 2) + \r\n    geom_smooth() +\r\n    labs(\r\n      title = \"Hour Record Pacing Strategies (Color: Athlete)\",\r\n      subtitle = \"Average Speed per Attempt displayed as dotted line\",\r\n      x = \"Lap Number\", y = \"Speed [km / h]\", color = \"Athlete\") +\r\n    theme(legend.position = \"bottom\")\r\n}\r\n\r\n\r\n\r\n\r\nIn the final plot you can see the different pacing strategies of the attempts.\r\n\r\n\r\n\r\n",
    "preview": "https://raw.githubusercontent.com/duju211/ganna_hour_record/master/file_out/gg_time_all.png",
    "last_modified": "2023-12-01T22:56:56+01:00",
    "input_file": {}
  },
  {
    "path": "posts/mountain-race/",
    "title": "Mountain Race",
    "description": "Animation of my Strava efforts on one of my local climbs",
    "author": [
      {
        "name": "Julian During",
        "url": {}
      }
    ],
    "date": "2022-03-30",
    "categories": [],
    "contents": "\r\nIdea\r\nEvery cyclist has a particular important climb. It might not be a big\r\ndeal to anyone else, but any climb can be important!\r\nMy favorite local climb goes by the name of ‘Lochen’. It’s located\r\noutside of my local hometown Balingen in the southwest of Germany. It’s\r\nabout 4.4 kilometers long with an average gradient of 6.9%.\r\nThis doesn’t sound like a hard climb. It might not even register as a\r\nregular big climb for most cyclists. But for me it’s one of the most\r\niconic climbs.\r\nIn the following post, I will let different versions of me race\r\nagainst each other on my favorite local climb!\r\nIn order to reproduce the analysis, perform the following steps:\r\nClone the repository\r\nInstall the packages listed in the libraries.R\r\nfile\r\nRun the target pipeline by executing\r\ntargets::tar_make() command\r\nData\r\nThe data originates from my personal Strava account. If you have a\r\nStrava account and want to query your data like I do here, you can have\r\na look at one of my previous\r\nposts.\r\nThe data are a bunch of arrow files, that you can query via dpylr\r\nsyntax thanks to the DuckDB package.\r\nDeselect heartrate measurements and restrict the spatial\r\ndata to a bounding box. Add information about the type and the start\r\ndate of each activity.\r\n\r\n\r\npoi <- function(df_act, paths_meas, target_file, act_type,\r\n                lng_min, lng_max, lat_min, lat_max) {\r\n  act_col_types <- schema(\r\n    moving = boolean(), velocity_smooth = double(),\r\n    grade_smooth = double(), distance = double(),\r\n    altitude = double(), heartrate = int32(), time = int32(),\r\n    lat = double(), lng = double(), cadence = int32(),\r\n    watts = int32(), id = string())\r\n\r\n  strava_db <- open_dataset(\r\n    paths_meas, format = \"arrow\", schema = act_col_types) |>\r\n    to_duckdb()\r\n\r\n  df_strava_poi <- strava_db |>\r\n    filter(\r\n      lng >= lng_min, lng <= lng_max, lat >= lat_min, lat <= lat_max) |>\r\n    select(-heartrate) |>\r\n    collect() |>\r\n    left_join(select(df_act, id, type, start_date), by = \"id\")\r\n}\r\n\r\n\r\n\r\n\r\n# A tibble: 40,439 × 13\r\n   moving velocity_smooth grade_smooth distance altitude  time   lat\r\n   <lgl>            <dbl>        <dbl>    <dbl>    <dbl> <int> <dbl>\r\n 1 TRUE               3            3.4   22882.     637.  3925  48.2\r\n 2 TRUE               3            1.7   22885.     637.  3926  48.2\r\n 3 TRUE               2.9          3.4   22887.     637   3927  48.2\r\n 4 TRUE               3            3.4   22890.     637   3928  48.2\r\n 5 TRUE               2.9          3.3   22893.     637.  3929  48.2\r\n 6 TRUE               3            3.3   22896.     637.  3930  48.2\r\n 7 TRUE               2.9          3.3   22899.     637.  3931  48.2\r\n 8 TRUE               3            5     22902.     637.  3932  48.2\r\n 9 TRUE               3            4.9   22905.     638.  3933  48.2\r\n10 TRUE               3            6.7   22908.     638.  3934  48.2\r\n# … with 40,429 more rows, and 6 more variables: lng <dbl>,\r\n#   cadence <int>, watts <int>, id <chr>, type <chr>,\r\n#   start_date <dttm>\r\n\r\nFurther preprocess the raw data. Keep only rows, where I was moving\r\nand turn the start date from datetime to date. Adjust the\r\ntime column so that every activity starts at time 0.\r\n\r\n\r\nlochen <- function(df_lochen_raw) {\r\n  df_lochen_raw |>\r\n    group_by(id) |>\r\n    mutate(\r\n      time_delta = time - lag(time),\r\n      start_date = as_date(start_date)) |>\r\n    replace_na(list(time_delta = 0)) |>\r\n    filter(moving) |>\r\n    mutate(time = cumsum(time_delta)) |>\r\n    select(-time_delta)\r\n}\r\n\r\n\r\n\r\n\r\n# A tibble: 40,365 × 13\r\n# Groups:   id [36]\r\n   moving velocity_smooth grade_smooth distance altitude  time   lat\r\n   <lgl>            <dbl>        <dbl>    <dbl>    <dbl> <int> <dbl>\r\n 1 TRUE               3            3.4   22882.     637.     0  48.2\r\n 2 TRUE               3            1.7   22885.     637.     1  48.2\r\n 3 TRUE               2.9          3.4   22887.     637      2  48.2\r\n 4 TRUE               3            3.4   22890.     637      3  48.2\r\n 5 TRUE               2.9          3.3   22893.     637.     4  48.2\r\n 6 TRUE               3            3.3   22896.     637.     5  48.2\r\n 7 TRUE               2.9          3.3   22899.     637.     6  48.2\r\n 8 TRUE               3            5     22902.     637.     7  48.2\r\n 9 TRUE               3            4.9   22905.     638.     8  48.2\r\n10 TRUE               3            6.7   22908.     638.     9  48.2\r\n# … with 40,355 more rows, and 6 more variables: lng <dbl>,\r\n#   cadence <int>, watts <int>, id <chr>, type <chr>,\r\n#   start_date <date>\r\n\r\nVisualisation\r\nMake a first static ggplot visualisation. Keep the plot rather\r\nminimal. Use ggplot2::theme_void as a general theme:\r\n\r\n\r\nvis_lochen <- function(df_lochen) {\r\n  df_lochen |>\r\n    ggplot(\r\n      aes(x = lng, y = lat, group = id)) +\r\n    geom_path(alpha = 0.2) +\r\n    theme(\r\n      axis.ticks.x = element_blank(), legend.position = \"bottom\") +\r\n    labs(x = element_blank(), y = element_blank(), color = \"Activity Year\")\r\n}\r\n\r\n\r\n\r\n\r\n\r\n\r\nAs you can see there are lot of paths on one road. These are my bike\r\nrides on the ‘Lochen’ pass.\r\nSome paths don’t seem to match. These are activities of another type\r\nin the same region as my bike rides. These activities don’t use the main\r\nroad and stand out in the plot.\r\nTo further explore the data, make a first animated visualisation with\r\nthe gganimate package:\r\n\r\n\r\nvis_anim_lochen <- function(gg_lochen) {\r\n  gg_lochen +\r\n    transition_reveal(along = time)\r\n}\r\n\r\n\r\n\r\n\r\n\r\n\r\nIn this animated version of the plot, you can see that there are\r\nfurther problems in the data. Not all bike rides start at the bottom of\r\nthe climb. You can guess which activities start at the top of the climb,\r\nby looking at the general speed of the animation. Determine these\r\nactivities:\r\n\r\n\r\nwrong_direction <- function(df_lochen) {\r\n  df_lat_lng_start <- df_lochen |>\r\n    group_by(id) |>\r\n    summarise(\r\n      start_lat = lat[time == min(time)], start_lng = lng[time == min(time)])\r\n  \r\n  midway <- min(df_lochen$lat) + (max(df_lochen$lat) - min(df_lochen$lat)) / 2\r\n  \r\n  df_lat_lng_start |>\r\n    filter(start_lat < midway)\r\n}\r\n\r\n\r\n\r\nFilter the activities for bike rides. Exclude activities that start\r\nat the top of the climb. Repeat the above animated plot:\r\n\r\n\r\nlochen_ride <- function(df_lochen, df_wrong_direction) {\r\n  df_lochen |>\r\n    filter(type == \"Ride\") |>\r\n    anti_join(df_wrong_direction, by = \"id\")\r\n}\r\n\r\n\r\n\r\n\r\n\r\n\r\nNow it looks much cleaner and the rides are more comparable to one\r\nanother.\r\nFor the final version of the animation, add small points to point out\r\nmy position at the time. Color these positions by the year\r\nof the activity Reduce the speed of the animation a little bit, to\r\ndisplay smaller differences in the rides.\r\n\r\n\r\nvis_anim_lochen_final <- function(gg_lochen_ride) {\r\n  gg_endpoints <- gg_lochen_ride +\r\n    geom_point(shape = 4, aes(color = as_factor(year(start_date))))\r\n  \r\n  gg_anim_endpoints <- vis_anim_lochen(gg_endpoints)\r\n  \r\n  animate(gg_anim_endpoints, fps = 7)\r\n}\r\n\r\n\r\n\r\n\r\n\r\n\r\nI very much like how the plot turned out. I hope I can add many more\r\nlayers to this animation in the future!\r\n\r\n\r\n\r\n",
    "preview": "https://github.com/duju211/mountain_race/blob/master/README_files/figure-markdown_strict/gg_lochen-1.png?raw=true",
    "last_modified": "2023-12-01T22:56:56+01:00",
    "input_file": {}
  },
  {
    "path": "posts/strava-data/",
    "title": "Strava Data",
    "description": "Article on how to effectively scrape and store Strava data using the `targets`\npackage",
    "author": [
      {
        "name": "Julian During",
        "url": {}
      }
    ],
    "date": "2022-03-03",
    "categories": [],
    "contents": "\r\nI am an avid runner and cyclist. For the past couple of years, I have recorded almost all my activities on some kind of GPS device.\r\nI record my runs with a Garmin device and my bike rides with a Wahoo device, and I synchronize both accounts on Strava. I figured that it would be nice to directly access my data from my Strava account.\r\nIn the following text, I will describe the progress to get Strava data into R, process the data, and then create a visualization of activity routes.\r\nYou will need the following packages:\r\n\r\n\r\nlibrary(tarchetypes)\r\nlibrary(conflicted)\r\nlibrary(tidyverse)\r\nlibrary(lubridate)\r\nlibrary(jsonlite)\r\nlibrary(targets)\r\nlibrary(httpuv)\r\nlibrary(arrow)\r\nlibrary(httr)\r\nlibrary(pins)\r\nlibrary(glue)\r\nlibrary(fs)\r\n\r\nconflict_prefer(\"filter\", \"dplyr\")\r\n\r\n\r\n\r\nData\r\nThe whole data pipeline is implemented with the help of the targets package. You can learn more about the package and its functionalities here.\r\nIn order to reproduce the analysis, perform the following steps:\r\nClone the repository: https://github.com/duju211/pin_strava\r\nInstall the packages listed in the libraries.R file\r\nRun the target pipeline by executing targets::tar_make() command\r\nFollow the instructions printed in the console\r\nTarget Plan\r\nWe will go through the most important targets in detail.\r\nOAuth Dance from R\r\nThe Strava API requires an ‘OAuth dance’, described below.\r\nCreate an OAuth Strava app\r\nTo get access to your Strava data from R, you must first create a Strava API. The steps are documented on the Strava Developer site. While creating the app, you’ll have to give it a name. In my case, I named it r_api.\r\nAfter you have created your personal API, you can find your Client ID and Client Secret variables in the Strava API settings. Save the Client ID as STRAVA_KEY and the Client Secret as STRAVA_SECRET in your R environment.\r\n\r\nYou can edit your R environment by running usethis::edit_r_environ(), saving the keys, and then restarting R.\r\nSTRAVA_KEY=<Client ID>\r\nSTRAVA_SECRET=<Client Secret>\r\nThe function define_strava_app shown below creates the OAuth app:\r\n\r\n\r\ndefine_strava_app <- function() {\r\n  if (Sys.getenv(\"STRAVA_KEY\") == \"\" | Sys.getenv(\"STRAVA_SECRET\") == \"\")\r\n    stop(str_glue(\r\n      \"Please set system variables 'STRAVA_KEY' and 'STRAVA_SECRET' before \",\r\n      \"continuing. How you can create these variables is described here: \",\r\n      \"https://developers.strava.com/docs/getting-started/. \",\r\n      \"You can set the system variables with the `usethis::edit_r_environ` \",\r\n      \"function.\"))\r\n\r\n  oauth_app(\r\n    appname = \"r_api\",\r\n    key = Sys.getenv(\"STRAVA_KEY\"),\r\n    secret = Sys.getenv(\"STRAVA_SECRET\"))\r\n}\r\n\r\n\r\n\r\nDefine an endpoint\r\nDefine an endpoint called my_endpoint using the function define_strava_endpoint.\r\nThe authorize parameter describes the authorization url and the access argument exchanges the authenticated token.\r\n\r\n\r\ndefine_strava_endpoint <- function() {\r\n  oauth_endpoint(\r\n    request = NULL,\r\n    authorize = \"https://www.strava.com/oauth/authorize\",\r\n    access = \"https://www.strava.com/oauth/token\")\r\n}\r\n\r\n\r\n\r\nThe final authentication step\r\nBefore you can execute the following steps, you have to authenticate the API in the web browser.\r\n\r\n\r\ndefine_strava_sig <- function(endpoint, app) {\r\n  oauth2.0_token(\r\n    endpoint, app,\r\n    scope = \"activity:read_all,activity:read,profile:read_all\",\r\n    type = NULL, use_oob = FALSE, as_header = FALSE,\r\n    use_basic_auth = FALSE, cache = FALSE)\r\n}\r\n\r\n\r\n\r\nThe information in my_sig can now be used to access Strava data. Set the cue_mode of the target to ‘always’ so that the following API calls are always executed with an up-to-date authorization token.\r\nCurrent authenticated user\r\nDownload information about the currently authenticated user. When preprocessing the data, the columns shoes, clubs and bikes need special attention, because they can contain multiple entries and can be interpreted as list columns.\r\n\r\n\r\nactive_user <- function(access_token, user_list_cols, meas_board) {\r\n  athlete_url <- parse_url(\"https://www.strava.com/api/v3/athlete\")\r\n\r\n  r <- athlete_url %>%\r\n    modify_url(\r\n      query = list(access_token = access_token)) %>%\r\n    GET()\r\n\r\n  user_list <- content(r, as = \"text\") %>%\r\n    fromJSON()\r\n\r\n  df_user <- user_list[\r\n    map_lgl(user_list, ~ !is.null(.x))\r\n    & map_lgl(names(user_list), ~ !(.x %in% user_list_cols))] %>%\r\n    as_tibble()\r\n\r\n  list_cols <- user_list[names(user_list) %in% user_list_cols] %>%\r\n    map(as_tibble)\r\n\r\n  for (i in seq_along(list_cols)) {\r\n    df_user[[names(list_cols)[[i]]]] <- list(list_cols[[i]])\r\n  }\r\n  df_user\r\n}\r\n\r\n\r\n\r\nIn the end there is a data frame with one row for the currently authenticated user:\r\n\r\n# A tibble: 1 x 27\r\n        id resource_state firstname lastname city  state country sex  \r\n     <int>          <int> <chr>     <chr>    <chr> <chr> <chr>   <chr>\r\n1 26845822              3 \"Julian \" During   Bali~ Bade~ Germany M    \r\n# ... with 19 more variables: premium <lgl>, summit <lgl>,\r\n#   created_at <chr>, updated_at <chr>, badge_type_id <int>,\r\n#   weight <dbl>, profile_medium <chr>, profile <chr>, blocked <lgl>,\r\n#   can_follow <lgl>, follower_count <int>, friend_count <int>,\r\n#   mutual_friend_count <int>, athlete_type <int>,\r\n#   date_preference <chr>, measurement_preference <chr>,\r\n#   clubs <list>, bikes <list>, shoes <list>\r\n\r\nActivities\r\nLoad a data frame that gives an overview of all the activities from the data. Because the total number of activities is unknown, use a while loop. It will break the execution of the loop if there are no more activities to read.\r\n\r\n\r\nread_all_activities <- function(access_token, active_user_id) {\r\n  activities_url <- parse_url(\r\n    \"https://www.strava.com/api/v3/athlete/activities\")\r\n\r\n  act_vec <- vector(mode = \"list\")\r\n  df_act <- tibble(init = \"init\")\r\n  i <- 1L\r\n\r\n  while (nrow(df_act) != 0) {\r\n    r <- activities_url %>%\r\n      modify_url(\r\n        query = list(\r\n          access_token = access_token,\r\n          page = i)) %>%\r\n      GET()\r\n\r\n    stop_for_status(r)\r\n\r\n    df_act <- content(r, as = \"text\") %>%\r\n      fromJSON(flatten = TRUE) %>%\r\n      as_tibble()\r\n    if (nrow(df_act) != 0)\r\n      act_vec[[i]] <- df_act\r\n    i <- i + 1L\r\n  }\r\n\r\n  act_vec %>%\r\n    bind_rows() %>%\r\n    mutate(\r\n      start_date = ymd_hms(start_date),\r\n      active_user_id = active_user_id)\r\n}\r\n\r\n\r\n\r\nThe resulting data frame consists of one row per activity:\r\n\r\n# A tibble: 648 x 62\r\n   resource_state name               distance moving_time elapsed_time\r\n            <int> <chr>                 <dbl>       <int>        <int>\r\n 1              2 \"Test manueller E~    3010         3600         3600\r\n 2              2 \"Test Yoga\"              0           61           61\r\n 3              2 \"Tieringen \\U0001~    8055.        3208         3269\r\n 4              2 \"Sunset \\U0001f30~    7543.        3107         3141\r\n 5              2 \"Hangen Sprint \"      9275.        3715         3871\r\n 6              2 \"\\U0001f499\\U0001~    7742.        3115         3161\r\n 7              2 \"22222 \\U0001f92a\"    7748.        3203         3273\r\n 8              2 \"Storm Run\"           6771.        2845         2915\r\n 9              2 \"Union Ride\"         44468         6730         7321\r\n10              2 \"February Nightca~    6701.        2839         2850\r\n# ... with 638 more rows, and 57 more variables:\r\n#   total_elevation_gain <dbl>, type <chr>, workout_type <int>,\r\n#   id <dbl>, start_date <dttm>, start_date_local <chr>,\r\n#   timezone <chr>, utc_offset <dbl>, location_city <lgl>,\r\n#   location_state <lgl>, location_country <chr>,\r\n#   achievement_count <int>, kudos_count <int>, comment_count <int>,\r\n#   athlete_count <int>, photo_count <int>, trainer <lgl>, ...\r\n\r\nMake sure that all ID columns have a character format and improve the column names.\r\n\r\n\r\npre_process_act <- function(df_act_raw, active_user_id, meas_board) {\r\n  df_act_raw %>%\r\n    mutate(across(contains(\"id\"), as.character)) %>%\r\n    rename(athlete_id = `athlete.id`)\r\n}\r\n\r\n\r\n\r\nExtract ids of all activities. Exclude activities which were recorded manually, because they don’t include additional data:\r\n\r\n\r\nrel_act_ids <- function(df_act) {\r\n  df_act %>%\r\n    filter(!manual) %>%\r\n    pull(id)\r\n}\r\n\r\n\r\n\r\nMeasurements\r\nA ‘stream’ is a nested list (JSON format) with all available measurements of the corresponding activity.\r\nTo get the available variables and turn the result into a data frame, define a helper function read_activity_stream. This function takes an ID of an activity and an authentication token, which you created earlier.\r\nPreprocess and unnest the data in this function. The column latlng needs special attention, because it contains latitude and longitude information. Separate the two measurements before unnesting all list columns.\r\n\r\n\r\nread_activity_stream <- function(id, active_user_id, access_token) {\r\n  act_url <- parse_url(stringr::str_glue(\r\n    \"https://www.strava.com/api/v3/activities/{id}/streams\"))\r\n\r\n  r <- modify_url(\r\n    act_url,\r\n    query = list(\r\n      access_token = access_token,\r\n      keys = str_glue(\r\n        \"distance,time,latlng,altitude,velocity_smooth,heartrate,cadence,\",\r\n        \"watts,temp,moving,grade_smooth\"))) %>%\r\n    GET()\r\n\r\n  stop_for_status(r)\r\n\r\n  df_stream_raw <- fromJSON(content(r, as = \"text\"), flatten = TRUE) %>%\r\n    as_tibble() %>%\r\n    mutate(id = id) %>%\r\n    pivot_wider(names_from = type, values_from = data)\r\n\r\n  if (\"latlng\" %in% colnames(df_stream_raw)) {\r\n    df_stream <- df_stream_raw %>%\r\n      mutate(\r\n        lat = map(\r\n          .x = latlng, .f = ~ .x[, 1]),\r\n        lng = map(\r\n          .x = latlng, .f = ~ .x[, 2])) %>%\r\n      select(-latlng)\r\n  } else {\r\n    df_stream <- df_stream_raw\r\n  }\r\n\r\n  df_stream %>%\r\n    unnest(where(is_list)) %>%\r\n    mutate(id = id)\r\n}\r\n\r\n\r\n\r\nDo this for every id. Pin the resulting data frames to the local strava_data_26845822 board as an file of type ‘arrow’. By doing so we can later effectively query the data.\r\n\r\nThe name of the board is determined by the currently logged in user and will have a different name, if you run the pipeline.\r\n\r\n\r\npin_meas <- function(act_id, active_user_id, access_token, meas_board) {\r\n  pin_name <- paste0(\"df_\", act_id, \"_\", active_user_id)\r\n  meas <- read_activity_stream(act_id, active_user_id, access_token)\r\n  pin_write(meas_board, meas, pin_name, type = \"arrow\")\r\n}\r\n\r\n\r\n\r\nVisualisation\r\nVisualize the final data by displaying the geospatial information in the data. Join all the activities into one data frame. To do this, get the paths to all the measurement files:\r\n\r\n\r\nmeas_paths <- function(board_name) {\r\n  dir_ls(\r\n    board_name, type = \"file\", regexp = \"df_\\\\d.*\\\\.arrow$\", recurse = TRUE)\r\n}\r\n\r\n\r\n\r\nInsert them all into a duckdb and select relevant columns:\r\n\r\n\r\nmeas_all <- function(paths_meas) {\r\n  act_col_types <- schema(\r\n    moving = boolean(), velocity_smooth = double(),\r\n    grade_smooth = double(), distance = double(),\r\n    altitude = double(), heartrate = int32(), time = int32(),\r\n    lat = double(), lng = double(), cadence = int32(),\r\n    watts = int32(), id = string())\r\n\r\n  open_dataset(paths_meas, format = \"arrow\", schema = act_col_types) %>%\r\n    to_duckdb() %>%\r\n    select(id, lat, lng) %>%\r\n    filter(!is.na(lat) & !is.na(lng)) %>%\r\n    collect()\r\n}\r\n\r\n\r\n\r\n\r\n# A tibble: 2,252,282 x 3\r\n   id           lat   lng\r\n   <chr>      <dbl> <dbl>\r\n 1 1327205128  48.2  9.02\r\n 2 1327205128  48.2  9.02\r\n 3 1327205128  48.2  9.02\r\n 4 1327205128  48.2  9.02\r\n 5 1327205128  48.2  9.02\r\n 6 1327205128  48.2  9.02\r\n 7 1327205128  48.2  9.02\r\n 8 1327205128  48.2  9.02\r\n 9 1327205128  48.2  9.02\r\n10 1327205128  48.2  9.02\r\n# ... with 2,252,272 more rows\r\n\r\nIn the final plot every facet is one activity. Keep the rest of the plot as minimal as possible.\r\n\r\n\r\nvis_meas <- function(df_meas_pro) {\r\n  df_meas_pro %>%\r\n    ggplot(aes(x = lng, y = lat)) +\r\n    geom_path() +\r\n    facet_wrap(~ id, scales = \"free\") +\r\n    theme(\r\n      axis.line = element_blank(),\r\n      axis.text.x = element_blank(),\r\n      axis.text.y = element_blank(),\r\n      axis.ticks = element_blank(),\r\n      axis.title.x = element_blank(),\r\n      axis.title.y = element_blank(),\r\n      legend.position = \"bottom\",\r\n      panel.background = element_blank(),\r\n      panel.border = element_blank(),\r\n      panel.grid.major = element_blank(),\r\n      panel.grid.minor = element_blank(),\r\n      plot.background = element_blank(),\r\n      strip.text = element_blank())\r\n}\r\n\r\n\r\n\r\n\r\n\r\n\r\nAnd there it is: All your Strava data in a few tidy data frames and a nice-looking plot. Future updates to the data shouldn’t take too long, because only measurements from new activities will be downloaded. With all your Strava data up to date, there are a lot of appealing possibilities for further data analyses of your fitness data.\r\n\r\n\r\n\r\n",
    "preview": "posts/strava-data/distill-preview.png",
    "last_modified": "2023-12-01T22:56:56+01:00",
    "input_file": {},
    "preview_width": 2100,
    "preview_height": 2100
  },
  {
    "path": "posts/hex_sticker/",
    "title": "Hex Sticker",
    "description": "How to create a hex sticker for a new package",
    "author": [
      {
        "name": "Julian During",
        "url": {}
      }
    ],
    "date": "2021-05-23",
    "categories": [],
    "contents": "\r\nIn this post, I want to describe how I created a hex sticker for one of my shiny apps. The app itself lets users interactively explore their Strava data.\r\nBecause I called the app SummitR, I decided to display one of the most famous summit finishes in cycling history: Alpe d’Huez.\r\nI am using the following packages:\r\n\r\n\r\nlibrary(hexSticker)\r\nlibrary(tidyverse)\r\nlibrary(pins)\r\n\r\n\r\n\r\nThe hexSticker package is used to create the final sticker.\r\nBecause I already climbed Alpe d’Huez on a previous occasion, I can get the data already prepared from a private github repository. The data in the corresponding repository is organised as a pin:\r\n\r\n\r\nalpe_dhuez_id <- \"1714646144\"\r\nathlete_id <- \"26845822\"\r\n\r\nboard_register_github(repo = \"duju211/strava_act\", branch = \"master\")\r\n\r\ndf_act <- pin_get(\r\n  str_glue(\"act_{alpe_dhuez_id}_{athlete_id}\"), board = \"github\")\r\n\r\nboard_disconnect(\"github\")\r\n\r\n\r\n\r\nIn raw table form the data of the activity looks like this:\r\n\r\n# A tibble: 5,326 x 11\r\n   moving velocity_smooth grade_smooth distance altitude  time   lat\r\n   <lgl>            <dbl>        <dbl>    <dbl>    <dbl> <int> <dbl>\r\n 1 FALSE              0            8.6      0       787      0  45.1\r\n 2 TRUE               0            8.6      2.1     787.     1  45.1\r\n 3 TRUE               2.3          8.6      4.5     787.     2  45.1\r\n 4 TRUE               2.3         10        7       788.     3  45.1\r\n 5 TRUE               2.3         11.9      9.3     788.     4  45.1\r\n 6 TRUE               2.3         12       11.6     788      5  45.1\r\n 7 TRUE               2.4         11.9     14.1     788.     6  45.1\r\n 8 TRUE               2.4         13.6     16.3     789.     7  45.1\r\n 9 TRUE               2.3         12       18.7     789      8  45.1\r\n10 TRUE               2.4         12.6     21.1     789.     9  45.1\r\n# ... with 5,316 more rows, and 4 more variables: lng <dbl>,\r\n#   heartrate <int>, cadence <int>, watts <int>\r\n\r\nCreate a minimalistic plot of the activity. Only take variables lat and lng into account:\r\n\r\n\r\ngg_act <- df_act %>% \r\n  ggplot(aes(x = lng, y = lat)) +\r\n  geom_path(color = \"white\") +\r\n  theme_void() +\r\n  theme(legend.position = \"none\")\r\n\r\n\r\n\r\nPlot the path of the activity on a black background:\r\n\r\n\r\ngg_act +\r\n  theme_dark()\r\n\r\n\r\n\r\n\r\nSave the finished hex sticker as result. Use the official Strava color as background color.\r\n\r\n\r\nsticker(\r\n  gg_act, package=\"SummitR\", p_size=20, s_x=1.1, s_y=.75, s_width=1.3,\r\n  s_height=1, h_fill = \"#fc4c02\", h_color = \"white\",\r\n  filename=\"_posts/hex_sticker/hex_sticker.png\")\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/hex_sticker/distill-preview.png",
    "last_modified": "2023-12-01T22:56:56+01:00",
    "input_file": {},
    "preview_width": 1161,
    "preview_height": 701
  },
  {
    "path": "posts/transalp/",
    "title": "Getting Over It",
    "description": "Visualising my Transalp bike ride",
    "author": [
      {
        "name": "Julian During",
        "url": {}
      }
    ],
    "date": "2021-05-23",
    "categories": [],
    "contents": "\r\nThis summer I crossed the alps with my road bike. I’ve recorded the whole ride and as a nice memory, I would like to visualise this ride.\r\nA short time ago I’ve discovered the awesome R package drake. The use of this package transformed the way I do my analysis and it helps me to make my post more reproducible. The following blog post describes the underlying workflow, after which I’ve developed the underlying package transalp for this post.\r\nData\r\nAt first you have to install the package from github. Then you have to load it.\r\n\r\n\r\nremotes::install_github(\"duju211/transalp\")\r\n\r\nlibrary(transalp)\r\n\r\n\r\n\r\nLoad the other necessary libraries:\r\n\r\n\r\nlibrary(tidyverse)\r\nlibrary(patchwork)\r\nlibrary(drake)\r\nlibrary(here)\r\nlibrary(fs)\r\n\r\n\r\n\r\nThe package includes the underlying data:\r\n\r\n\r\ndf_act_meas_raw <- transalp::df_act_meas\r\n\r\n\r\n\r\n\r\n# A tibble: 90,917 x 10\r\n   id         act_date   moving velocity_smooth grade_smooth distance\r\n   <chr>      <date>     <lgl>            <dbl>        <dbl>    <dbl>\r\n 1 3669729902 2020-06-25 FALSE              0            0        0  \r\n 2 3669729902 2020-06-25 FALSE              0            0        0.9\r\n 3 3669729902 2020-06-25 TRUE               0          -44.6      4.6\r\n 4 3669729902 2020-06-25 TRUE               0            1.4      8.4\r\n 5 3669729902 2020-06-25 TRUE               0            2.8     12.1\r\n 6 3669729902 2020-06-25 TRUE               0            2.8     15.6\r\n 7 3669729902 2020-06-25 TRUE               0            1.4     19.1\r\n 8 3669729902 2020-06-25 TRUE               3.6          1.4     22.5\r\n 9 3669729902 2020-06-25 TRUE               3.5          0       26  \r\n10 3669729902 2020-06-25 TRUE               3.5          1.6     29.5\r\n# ... with 90,907 more rows, and 4 more variables: altitude <dbl>,\r\n#   time <int>, lat <dbl>, lng <dbl>\r\n\r\nPreprocessing\r\nLook at the first function:\r\n\r\n\r\n#' Function to preprocess measurements of the included activities.\r\n#'\r\n#' @param df_act_meas\r\n#'\r\n#' @return Preprocessed activities\r\n#' @export\r\n#'\r\n#' @examples\r\n#' pre_process_meas(df_act_meas)\r\npre_process_meas <- function(df_act_meas) {\r\n  df_act_meas %>%\r\n    dplyr::mutate(\r\n      act_date_chr = as.character(act_date),\r\n      altitude_norm = altitude / max(altitude)) %>%\r\n    dplyr::group_by(id) %>%\r\n    dplyr::mutate(distance_norm = distance / max(distance)) %>%\r\n    dplyr::ungroup()\r\n}\r\n\r\n\r\n\r\nThe function does some basic preprocessing on the included activities:\r\nTurn activity date into character (for easier plotting)\r\nNormalize altitude and distance\r\n\r\n\r\ndf_act_meas <- pre_process_meas(df_act_meas_raw)\r\n\r\n\r\n\r\nNest the data frame by id and act_date_chr. Create a new sf column with the geospatial information of the activities:\r\n\r\n\r\n#' Turn every activity into an sf object. Nest the data frame by 'id' and\r\n#' 'act_date_chr' to do this.\r\n#'\r\n#' @param df_act_meas\r\n#'\r\n#' @return sf object\r\n#' @export\r\n#'\r\n#' @examples\r\nconvert_to_sf <- function(df_act_meas) {\r\n  df_act_meas %>%\r\n    tidyr::nest(act_data = -c(id, act_date_chr)) %>%\r\n    dplyr::mutate(\r\n      line = purrr::map(\r\n        act_data,\r\n        ~ sf::st_linestring(as.matrix(.x[, c(\"lng\", \"lat\", \"altitude\")]))),\r\n      geom = purrr::map(line, sf::st_sfc, crs = 4326)) %>%\r\n    sf::st_as_sf()\r\n}\r\n\r\n\r\n\r\n\r\n\r\nsf_act_meas <- convert_to_sf(df_act_meas)\r\n\r\n\r\n\r\n\r\nSimple feature collection with 5 features and 4 fields\r\nGeometry type: LINESTRING\r\nDimension:     XYZ\r\nBounding box:  xmin: 8.317612 ymin: 46.0061 xmax: 9.035269 ymax: 48.21307\r\nz_range:       zmin: 206.8 zmax: 2477.6\r\nCRS:           NA\r\n# A tibble: 5 x 5\r\n  id     act_date_chr act_data                             line geom  \r\n  <chr>  <chr>        <list>                       <LINESTRING> <list>\r\n1 36697~ 2020-06-25   <tibble [~ Z (8.596761 46.63501 1451.6, ~ <LINE~\r\n2 36646~ 2020-06-24   <tibble [~ Z (8.602206 46.63612 1510.2, ~ <LINE~\r\n3 36590~ 2020-06-23   <tibble [~ Z (8.625703 46.90152 447, 8.6~ <LINE~\r\n4 36542~ 2020-06-22   <tibble [~ Z (8.741265 47.49397 432, 8.7~ <LINE~\r\n5 36504~ 2020-06-21   <tibble [~ Z (9.021049 48.21307 750.2, 9~ <LINE~\r\n\r\nExtract the start point of every tour except for the last one. Extract the end point for this tour.\r\n\r\n\r\n#' Extract points of interest.\r\n#'\r\n#' @param sf_act_meas\r\n#'\r\n#' @return Data frame with points of interest\r\n#' @export\r\n#'\r\n#' @examples\r\nextract_poi <- function(sf_act_meas) {\r\n  sf_act_meas %>%\r\n    tibble::as_tibble() %>%\r\n    dplyr::transmute(\r\n      id, first_row = purrr::map(act_data, ~ .x[1,]),\r\n      last_row = purrr::map(act_data, ~ .x[nrow(.x), ]),\r\n      decisive_row = dplyr::if_else(id == \"3669729902\", last_row, first_row)) %>%\r\n    tidyr::unnest(decisive_row) %>%\r\n    dplyr::select(where(purrr::negate(purrr::is_list))) %>%\r\n    dplyr::mutate(\r\n      poi_name = dplyr::case_when(\r\n        act_date == \"2020-06-21\" ~ \"Albstadt\",\r\n        act_date == \"2020-06-22\" ~ \"Winterthur\",\r\n        act_date == \"2020-06-23\" ~ \"Flüelen\",\r\n        act_date == \"2020-06-24\" ~ \"Andermatt\",\r\n        act_date == \"2020-06-25\" ~ \"Lugano\",\r\n        TRUE ~ NA_character_)) %>%\r\n    dplyr::rename(lon = lng)\r\n}\r\n\r\n\r\n\r\n\r\n\r\ndf_poi <- extract_poi(sf_act_meas)\r\n\r\n\r\n\r\nVisualisation\r\nAltitude\r\nVisualise the altitude data:\r\n\r\n\r\n#' Visualise the altitude by using a ridge plot.\r\n#'\r\n#' @param df_act_meas\r\n#'\r\n#' @return ggplot of altitude data\r\n#' @export\r\n#'\r\n#' @examples\r\nvis_altitude_ridge <- function(df_act_meas) {\r\n  df_act_meas %>%\r\n    tibble::as_tibble() %>%\r\n    tidyr::unnest_legacy(act_data) %>%\r\n    dplyr::mutate(distance = distance / 1000) %>%\r\n    ggplot2::ggplot(ggplot2::aes(\r\n      x = distance, y = act_date_chr, height = altitude,\r\n      group = act_date_chr, color = act_date_chr)) +\r\n    ggridges::geom_ridgeline(scale = 0.0025, alpha = 0.2) +\r\n    ggplot2::labs(\r\n      x = \"Distance [km]\", y = ggplot2::element_blank()) +\r\n    ggplot2::theme_light() +\r\n    ggplot2::theme(\r\n      legend.position = \"none\") +\r\n    ggplot2::scale_y_discrete(position = \"right\") +\r\n    ggplot2::scale_x_continuous(breaks = scales::breaks_width(10))\r\n}\r\n\r\n\r\n\r\nThe function mainly applies the ggridges::geom_ridgeline function to the data. A form of visualisation that I’ve already used a lot. Its style is reminiscent of the famous Joy Division album cover.\r\n\r\n\r\ngg_altitude_ridges <- vis_altitude_ridge(sf_act_meas)\r\n\r\n\r\n\r\n\r\n\r\n\r\nSpatial Data\r\nDetermine the map data for all the activities. Use the ggmap::get_stamenmap function to download the data. The bounding box is calculated from the sf_act_meas object.\r\n\r\n\r\n#' Get the ground map for the visualisation of the spatial data.\r\n#'\r\n#' @param sf_act_meas\r\n#' @param tol_bbox\r\n#' @param map_zoom\r\n#'\r\n#' @return\r\n#' @export\r\n#'\r\n#' @examples\r\nget_alpen_map <- function(sf_act_meas, tol_bbox = 0.015, map_zoom = 10) {\r\n  bbox <- sf::st_bbox(sf_act_meas)\r\n\r\n  stamen_map <- ggmap::get_stamenmap(\r\n    bbox = c(\r\n      left = bbox$xmin[[1]] - tol_bbox,\r\n      right = bbox$xmax[[1]] + tol_bbox,\r\n      bottom = bbox$ymin[[1]] - tol_bbox,\r\n      top = bbox$ymax[[1]] + tol_bbox),\r\n    maptype = \"terrain-background\", zoom = map_zoom, color = \"bw\")\r\n}\r\n\r\n\r\n\r\n\r\n\r\ngg_alpen <- get_alpen_map(sf_act_meas, tol_bbox = 0.1, map_zoom = 9)\r\n\r\n\r\n\r\n\r\n\r\n\r\nPlot the activity and the point of interest data onto the map. Use the ggrepel package to plot the labels of the points of interest. This avoids too much overplotting.\r\n\r\n\r\n#' Plot the spatial data (route + point of interest) onto the map\r\n#'\r\n#' @param sf_act_meas\r\n#' @param gg_alpen\r\n#'\r\n#' @return ggplot of spatial data\r\n#' @export\r\n#'\r\n#' @examples\r\nvis_ride <- function(sf_act_meas, gg_alpen, df_poi) {\r\n  ggmap::ggmap(gg_alpen) +\r\n    ggplot2::geom_sf(\r\n      data = sf_act_meas, inherit.aes = FALSE,\r\n      mapping = aes(color = act_date_chr), size = 1.2) +\r\n    ggrepel::geom_label_repel(\r\n      data = df_poi, mapping = aes(label = poi_name), alpha = 0.6,\r\n      family = \"Fira Code Retina\", size = 2.5) +\r\n    ggplot2::theme_light() +\r\n    ggplot2::labs(\r\n      x = \"Longitude\", y = \"Latitude\") +\r\n    ggplot2::theme(legend.position = \"none\")\r\n}\r\n\r\n\r\n\r\n\r\n\r\ngg_rides <- vis_ride(sf_act_meas, gg_alpen_map, df_poi)\r\n\r\n\r\n\r\n\r\n\r\n\r\nCombine Visualisations\r\nCombine everything into one big plot using the patchwork package:\r\n\r\n\r\n# extrafont::font_import()\r\nextrafont::loadfonts(device = \"win\")\r\n\r\nfinal_plot <- (gg_rides + gg_altitude_ridges) +\r\n      plot_annotation(\r\n        title = \"Transalp 2020\",\r\n        subtitle = \"Albstadt - Lugano\") &\r\n      theme(text = element_text(family = \"Fira Code Retina\"))\r\n\r\nfinal_plot\r\n\r\n\r\n\r\n\r\nThe extrafont package helps with using some fancy fonts. The font_import function has to be called once. Comment the function call in the above code chunk because of this.\r\n\r\n\r\n\r\n",
    "preview": "posts/transalp/distill-preview.png",
    "last_modified": "2023-12-01T22:56:56+01:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  }
]
